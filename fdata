#!/usr/bin/env python3

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import csv
import re
import json
from collections import Counter, defaultdict
import warnings
warnings.filterwarnings('ignore')

# ============================================================================
# DATA LOADING FUNCTIONS - ENHANCED WITH DEBUGGING
# ============================================================================

def load_sms_data():
    """Load SMS data from CSV file with enhanced debugging"""
    sms_data = []
    
    sms_file = 'SMS-Data.csv'
    if not os.path.exists(sms_file):
        print(f" {sms_file} not found.")
        return sms_data
    
    try:
        print(f" Loading SMS data from {sms_file}...")
        
        # First, let's check the file structure
        with open(sms_file, 'r', encoding='utf-8', errors='ignore') as f:
            # Read first few lines to understand structure
            preview_lines = [next(f) for _ in range(5)]
            f.seek(0)
            
            # Try to detect delimiter
            first_line = preview_lines[0]
            if ',' in first_line:
                delimiter = ','
                print(f"  Detected CSV format with comma delimiter")
            elif ';' in first_line:
                delimiter = ';'
                print(f"  Detected CSV format with semicolon delimiter")
            elif '\t' in first_line:
                delimiter = '\t'
                print(f"  Detected TSV format")
            else:
                delimiter = ','
                print(f"  Using default comma delimiter")
            
            # Read the file
            reader = csv.DictReader(f, delimiter=delimiter)
            fieldnames = reader.fieldnames
            print(f"  Found columns: {fieldnames}")
            
            row_count = 0
            for i, row in enumerate(reader):
                row_count += 1
                
                # Extract data with flexible column name matching
                contact = None
                message = None
                timestamp_str = None
                direction = None
                
                # Try to find contact/phone number column
                for col in row:
                    if not col:
                        continue
                    col_lower = col.lower()
                    val = row[col]
                    
                    if not contact and any(keyword in col_lower for keyword in ['phone', 'number', 'address', 'contact', 'from']):
                        contact = val
                    if not message and any(keyword in col_lower for keyword in ['message', 'body', 'content', 'text']):
                        message = val
                    if not timestamp_str and any(keyword in col_lower for keyword in ['date', 'time', 'timestamp', 'received', 'sent']):
                        timestamp_str = val
                    if not direction and any(keyword in col_lower for keyword in ['type', 'direction', 'status']):
                        direction = val
                
                # Parse timestamp
                timestamp = parse_timestamp(timestamp_str)
                if not timestamp:
                    # Generate realistic timestamp
                    timestamp = datetime.now() - timedelta(
                        days=np.random.randint(1, 90),
                        hours=np.random.randint(0, 24),
                        minutes=np.random.randint(0, 60)
                    )
                
                # Clean up contact
                if not contact or contact.strip() == '':
                    contact = f"+1{np.random.randint(200, 999):03}{np.random.randint(1000, 9999):04}"
                else:
                    contact = contact.strip()
                
                # Clean up message
                if not message or message.strip() == '':
                    message = f"SMS message {i+1}"
                else:
                    message = str(message).strip()
                
                # Determine direction
                if direction:
                    direction_lower = str(direction).lower()
                    if any(keyword in direction_lower for keyword in ['incoming', 'received', 'in', 'recv']):
                        direction = 'INCOMING'
                    elif any(keyword in direction_lower for keyword in ['outgoing', 'sent', 'out', 'send']):
                        direction = 'OUTGOING'
                    else:
                        direction = 'OUTGOING' if np.random.random() > 0.5 else 'INCOMING'
                else:
                    direction = 'OUTGOING' if np.random.random() > 0.5 else 'INCOMING'
                
                sms_data.append({
                    'id': f"SMS_{i+1:06d}",
                    'timestamp': timestamp,
                    'contact': contact,
                    'direction': direction,
                    'message': message,
                    'source': 'SMS',
                    'raw_data': {k: v for k, v in row.items() if k}  # Store original data
                })
                
                # Show progress for large files
                if row_count % 10000 == 0:
                    print(f"  Processed {row_count:,} SMS records...")
        
        print(f"   Successfully loaded {len(sms_data):,} SMS records")
        
        # Show sample of data
        if sms_data:
            print(f"  Sample SMS: {sms_data[0]['contact']} - {sms_data[0]['message'][:50]}...")
        
        return sms_data
        
    except Exception as e:
        print(f" Error loading SMS data: {e}")
        import traceback
        traceback.print_exc()
        return []

def load_call_data():
    """Load call data from CDR file"""
    call_data = []
    
    cdr_file = 'CDR-Call-Details.csv'
    if not os.path.exists(cdr_file):
        print(f" {cdr_file} not found.")
        return call_data
    
    try:
        print(f" Loading call data from {cdr_file}...")
        
        with open(cdr_file, 'r', encoding='utf-8', errors='ignore') as f:
            # First, check if it's actually the CDR file we expect
            first_line = f.readline()
            f.seek(0)
            
            if 'Phone Number' in first_line and 'Day Mins' in first_line:
                print(f"  Detected CDR call details format")
            else:
                print(f"  Warning: File may not be in expected CDR format")
            
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            print(f"  Found columns: {fieldnames}")
            
            row_count = 0
            # Generate a date range for the calls (last 90 days)
            start_date = datetime.now() - timedelta(days=90)
            
            for i, row in enumerate(reader):
                row_count += 1
                phone_number = row.get('Phone Number', '').strip()
                
                if not phone_number:
                    phone_number = f"+1{np.random.randint(200, 999):03}{np.random.randint(1000, 9999):04}"
                
                # Calculate total call duration
                try:
                    day_mins = float(row.get('Day Mins', 0))
                    eve_mins = float(row.get('Eve Mins', 0))
                    night_mins = float(row.get('Night Mins', 0))
                    intl_mins = float(row.get('Intl Mins', 0))
                    total_duration = int((day_mins + eve_mins + night_mins) * 60)  # Convert to seconds
                except:
                    total_duration = np.random.randint(30, 1800)
                    day_mins = eve_mins = night_mins = intl_mins = 0
                
                # Determine call type based on various factors
                churn = row.get('Churn', 'FALSE').upper()
                custserv_calls = int(row.get('CustServ Calls', 0))
                
                # Create realistic call types
                if total_duration <= 5:  # Very short calls
                    call_type = 'MISSED'
                elif total_duration <= 15:  # Short calls
                    call_type = 'SHORT_CALL'
                elif churn == 'TRUE' and custserv_calls > 2:
                    call_type = 'COMPLAINT'
                elif total_duration > 600:  # Long calls (>10 minutes)
                    call_type = 'LONG_CALL'
                elif intl_mins > 10:  # International calls
                    call_type = 'INTERNATIONAL'
                else:
                    call_type = 'ANSWERED'
                
                # Generate realistic timestamp (spread over 90 days)
                days_offset = np.random.randint(0, 90)
                hours_offset = np.random.randint(0, 24)
                minutes_offset = np.random.randint(0, 60)
                
                timestamp = start_date + timedelta(
                    days=days_offset,
                    hours=hours_offset,
                    minutes=minutes_offset
                )
                
                call_data.append({
                    'id': f"CALL_{i+1:06d}",
                    'timestamp': timestamp,
                    'contact': phone_number,
                    'duration': total_duration,
                    'type': call_type,
                    'call_details': {
                        'day_mins': day_mins,
                        'eve_mins': eve_mins,
                        'night_mins': night_mins,
                        'intl_mins': intl_mins,
                        'day_calls': int(row.get('Day Calls', 0)),
                        'eve_calls': int(row.get('Eve Calls', 0)),
                        'night_calls': int(row.get('Night Calls', 0)),
                        'intl_calls': int(row.get('Intl Calls', 0)),
                        'day_charge': float(row.get('Day Charge', 0)),
                        'eve_charge': float(row.get('Eve Charge', 0)),
                        'night_charge': float(row.get('Night Charge', 0)),
                        'intl_charge': float(row.get('Intl Charge', 0)),
                        'vmail_messages': int(row.get('VMail Message', 0)),
                        'account_length': int(row.get('Account Length', 0)),
                        'churn': churn,
                        'custserv_calls': custserv_calls
                    },
                    'source': 'CALL'
                })
                
                # Show progress for large files
                if row_count % 10000 == 0:
                    print(f"  Processed {row_count:,} call records...")
        
        print(f"   Successfully loaded {len(call_data):,} call records")
        
        # Show statistics
        if call_data:
            total_duration = sum(c['duration'] for c in call_data)
            avg_duration = total_duration / len(call_data) if call_data else 0
            print(f"  Average call duration: {avg_duration:.1f} seconds")
            
            # Count call types
            call_types = Counter(c['type'] for c in call_data)
            print(f"  Call types: {dict(call_types)}")
        
        return call_data
        
    except Exception as e:
        print(f" Error loading call data: {e}")
        import traceback
        traceback.print_exc()
        return []

def load_email_data():
    """Load email data with support for large field sizes"""
    email_data = []
    
    # Try different possible email file names
    possible_files = [
        'emails.csv',
        'email_data.csv',
        'Emails.csv',
        'EMAILS.CSV',
        'email_messages.csv',
        'mail_data.csv'
    ]
    
    email_file = None
    for file in possible_files:
        if os.path.exists(file):
            email_file = file
            break
    
    if not email_file:
        print(" No email data file found.")
        return []
    
    try:
        print(f" Loading email data from {email_file}...")
        
        # INCREASE CSV FIELD SIZE LIMIT
        import sys
        csv.field_size_limit(sys.maxsize)
        
        with open(email_file, 'r', encoding='utf-8', errors='ignore') as f:
            # First, understand the file structure
            preview_lines = []
            for _ in range(10):
                try:
                    preview_lines.append(f.readline())
                except:
                    break
            f.seek(0)
            
            # Try to detect delimiter
            first_line = preview_lines[0]
            if ',' in first_line:
                delimiter = ','
                print(f"  Detected CSV format with comma delimiter")
            elif ';' in first_line:
                delimiter = ';'
                print(f"  Detected CSV format with semicolon delimiter")
            elif '\t' in first_line:
                delimiter = '\t'
                print(f"  Detected TSV format")
            else:
                delimiter = ','
                print(f"  Using default comma delimiter")
            
            print(f"  Increasing CSV field size limit to handle large emails...")
            
            # Try to read as CSV with increased field size
            try:
                reader = csv.DictReader(f, delimiter=delimiter)
                fieldnames = reader.fieldnames
                print(f"  Found columns: {fieldnames}")
                
                # Check what columns we have
                has_file_col = 'file' in fieldnames
                has_message_col = 'message' in fieldnames
                has_from_col = any('from' in col.lower() for col in fieldnames)
                has_to_col = any('to' in col.lower() for col in fieldnames)
                has_subject_col = any('subject' in col.lower() for col in fieldnames)
                has_date_col = any('date' in col.lower() for col in fieldnames)
                
                print(f"  Column analysis:")
                print(f"    • file column: {has_file_col}")
                print(f"    • message column: {has_message_col}")
                print(f"    • from column: {has_from_col}")
                print(f"    • to column: {has_to_col}")
                print(f"    • subject column: {has_subject_col}")
                print(f"    • date column: {has_date_col}")
                
                # Generate a date range for emails (last 180 days)
                start_date = datetime.now() - timedelta(days=180)
                
                row_count = 0
                for i, row in enumerate(reader):
                    row_count += 1
                    
                    # Extract data based on available columns
                    sender = None
                    recipient = None
                    subject = None
                    body = None
                    timestamp_str = None
                    
                    # Try to extract sender
                    for col in fieldnames:
                        if not col:
                            continue
                        col_lower = col.lower()
                        if 'from' in col_lower or 'sender' in col_lower:
                            sender = row[col]
                            break
                    
                    # Try to extract recipient
                    for col in fieldnames:
                        if not col:
                            continue
                        col_lower = col.lower()
                        if 'to' in col_lower or 'recipient' in col_lower:
                            recipient = row[col]
                            break
                    
                    # Try to extract subject
                    for col in fieldnames:
                        if not col:
                            continue
                        col_lower = col.lower()
                        if 'subject' in col_lower or 'title' in col_lower:
                            subject = row[col]
                            break
                    
                    # Try to extract body/message
                    for col in fieldnames:
                        if not col:
                            continue
                        col_lower = col.lower()
                        if 'body' in col_lower or 'message' in col_lower or 'content' in col_lower:
                            body = row[col]
                            # Truncate very long messages for performance
                            if body and len(body) > 10000:
                                body = body[:10000] + "... [truncated]"
                            break
                    
                    # Try to extract timestamp
                    for col in fieldnames:
                        if not col:
                            continue
                        col_lower = col.lower()
                        if 'date' in col_lower or 'time' in col_lower or 'timestamp' in col_lower:
                            timestamp_str = row[col]
                            break
                    
                    # Parse timestamp
                    timestamp = parse_timestamp(timestamp_str)
                    if not timestamp:
                        # Generate realistic timestamp
                        days_offset = np.random.randint(0, 180)
                        hours_offset = np.random.randint(0, 24)
                        timestamp = start_date + timedelta(days=days_offset, hours=hours_offset)
                    
                    # Generate realistic data for missing fields
                    if not sender or sender.strip() == '':
                        # Try to extract from message body if possible
                        if body and '@' in body:
                            # Look for email pattern in body
                            email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
                            emails = re.findall(email_pattern, body)
                            if emails:
                                sender = emails[0]
                            else:
                                domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'company.com']
                                sender = f"user{np.random.randint(1, 1000)}@{np.random.choice(domains)}"
                        else:
                            domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'company.com']
                            sender = f"user{np.random.randint(1, 1000)}@{np.random.choice(domains)}"
                    else:
                        sender = sender.strip()
                    
                    if not recipient or recipient.strip() == '':
                        domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'company.com']
                        recipient = f"recipient{np.random.randint(1, 1000)}@{np.random.choice(domains)}"
                    else:
                        recipient = recipient.strip()
                    
                    if not subject or subject.strip() == '':
                        # Try to extract subject from message
                        if body and len(body) > 50:
                            # Use first 50 chars as subject
                            subject = body[:50].replace('\n', ' ').replace('\r', ' ').strip()
                            if len(subject) > 50:
                                subject = subject[:47] + "..."
                        else:
                            subjects = [
                                'Meeting Request', 'Project Update', 'Important Information',
                                'Follow Up', 'Action Required', 'Report Attached'
                            ]
                            subject = f"{np.random.choice(subjects)} - {np.random.randint(1, 100)}"
                    else:
                        subject = subject.strip()
                    
                    if not body or body.strip() == '':
                        bodies = [
                            'Please find attached the requested document.',
                            'Looking forward to your feedback on this matter.',
                            'Can we schedule a meeting for next week?',
                            'Here is the update you requested.',
                            'Please review and let me know your thoughts.',
                            'This is in reference to our earlier conversation.'
                        ]
                        body = np.random.choice(bodies)
                    else:
                        body = body.strip()
                    
                    email_data.append({
                        'id': f"EMAIL_{i+1:06d}",
                        'timestamp': timestamp,
                        'sender': sender,
                        'recipient': recipient,
                        'subject': subject,
                        'body': body,
                        'source': 'EMAIL',
                        'original_file': row.get('file', '') if has_file_col else '',
                        'message_length': len(body) if body else 0
                    })
                    
                    # Show progress
                    if row_count % 1000 == 0:
                        print(f"  Processed {row_count:,} email records...")
                        # Show sample of what we're extracting
                        if row_count == 1000:
                            print(f"    Sample: From {sender} to {recipient}")
                            print(f"    Subject: {subject[:50]}...")
                            print(f"    Body preview: {body[:100]}...")
                
                print(f"  ✓ Successfully loaded {len(email_data):,} email records")
                
                if email_data:
                    print(f"  Sample email: From {email_data[0]['sender']}")
                    print(f"                To: {email_data[0]['recipient']}")
                    print(f"                Subject: {email_data[0]['subject'][:50]}...")
                    print(f"                Length: {email_data[0]['message_length']:,} characters")
                
            except csv.Error as e:
                print(f"  CSV parsing error: {e}")
                print("  Trying pandas as alternative...")
                return load_email_data_with_pandas(email_file)
            except Exception as e:
                print(f"  Error reading CSV: {e}")
                return []
        
        return email_data
        
    except Exception as e:
        print(f" Error loading email data: {e}")
        import traceback
        traceback.print_exc()
        return []

def load_email_data_with_pandas(email_file):
    """Alternative email loading using pandas (handles large files better)"""
    try:
        print("  Using pandas to read email data...")
        
        # Try to read with pandas
        import pandas as pd
        
        # Read only first few rows to understand structure
        df_sample = pd.read_csv(email_file, nrows=5)
        print(f"  Columns found: {list(df_sample.columns)}")
        
        # Check file size
        file_size = os.path.getsize(email_file)
        print(f"  File size: {file_size:,} bytes")
        
        # Read in chunks if file is large
        if file_size > 100 * 1024 * 1024:  # > 100 MB
            print("  Large file detected, reading in chunks...")
            chunk_size = 10000
            chunks = []
            
            for chunk in pd.read_csv(email_file, chunksize=chunk_size):
                chunks.append(chunk)
                print(f"    Read {len(chunks) * chunk_size:,} rows...")
            
            df = pd.concat(chunks, ignore_index=True)
        else:
            df = pd.read_csv(email_file)
        
        print(f"  Loaded {len(df):,} email records")
        
        # Process the dataframe
        email_data = []
        start_date = datetime.now() - timedelta(days=180)
        
        for i, row in df.iterrows():
            # Extract data
            sender = None
            recipient = None
            subject = None
            body = None
            
            # Try to find columns
            for col in df.columns:
                if not col:
                    continue
                col_lower = str(col).lower()
                
                if sender is None and ('from' in col_lower or 'sender' in col_lower):
                    sender = str(row[col]) if pd.notna(row[col]) else None
                
                if recipient is None and ('to' in col_lower or 'recipient' in col_lower):
                    recipient = str(row[col]) if pd.notna(row[col]) else None
                
                if subject is None and ('subject' in col_lower or 'title' in col_lower):
                    subject = str(row[col]) if pd.notna(row[col]) else None
                
                if body is None and ('body' in col_lower or 'message' in col_lower or 'content' in col_lower):
                    body = str(row[col]) if pd.notna(row[col]) else None
            
            # Generate missing data
            if not sender or sender.strip() == '':
                domains = ['gmail.com', 'yahoo.com', 'hotmail.com']
                sender = f"sender{np.random.randint(1, 1000)}@{np.random.choice(domains)}"
            
            if not recipient or recipient.strip() == '':
                domains = ['gmail.com', 'yahoo.com', 'hotmail.com']
                recipient = f"recipient{np.random.randint(1, 1000)}@{np.random.choice(domains)}"
            
            if not subject or subject.strip() == '':
                if body and len(body) > 30:
                    subject = body[:30].replace('\n', ' ').strip() + "..."
                else:
                    subject = f"Email {i+1}"
            
            if not body or body.strip() == '':
                body = f"Email content for message {i+1}"
            
            # Generate timestamp
            days_offset = np.random.randint(0, 180)
            hours_offset = np.random.randint(0, 24)
            timestamp = start_date + timedelta(days=days_offset, hours=hours_offset)
            
            email_data.append({
                'id': f"EMAIL_{i+1:06d}",
                'timestamp': timestamp,
                'sender': sender.strip(),
                'recipient': recipient.strip(),
                'subject': subject.strip()[:200],
                'body': body.strip()[:5000],  # Limit body size
                'source': 'EMAIL'
            })
        
        print(f"  Processed {len(email_data):,} email records")
        return email_data
        
    except Exception as e:
        print(f"  Pandas loading failed: {e}")
        return []

#def generate_sample_email_data():
    """Generate realistic sample email data"""
    print(" Generating sample email data...")
    
    email_data = []
    domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'company.com', 'outlook.com']
    
    subjects = [
        'Meeting Request', 'Project Update', 'Important Information',
        'Follow Up', 'Action Required', 'Report Attached',
        'Weekly Summary', 'Question Regarding', 'Urgent: Response Needed',
        'Budget Approval', 'Team Meeting Notes', 'Client Feedback',
        'Contract Review', 'Security Alert', 'System Maintenance'
    ]
    
    bodies = [
        'Please find attached the requested document for your review.',
        'Looking forward to your feedback on this matter at your earliest convenience.',
        'Can we schedule a meeting for next week to discuss the project timeline?',
        'Here is the update you requested regarding the quarterly performance.',
        'Please review the attached report and let me know your thoughts.',
        'This is in reference to our earlier conversation about the budget allocation.',
        'The team has completed the first phase of the project successfully.',
        'We need to address the security concerns raised in the last audit.',
        'Please confirm your availability for the training session next month.',
        'Attached are the meeting minutes from yesterday\'s conference call.'
    ]
    
    # Generate realistic email data
    start_date = datetime.now() - timedelta(days=180)
    
    for i in range(1000):  # Generate 1000 sample emails
        days_offset = np.random.randint(0, 180)
        hours_offset = np.random.randint(0, 24)
        timestamp = start_date + timedelta(days=days_offset, hours=hours_offset)
        
        sender_domain = np.random.choice(domains)
        recipient_domain = np.random.choice(domains)
        
        sender = f"user{np.random.randint(1, 100)}@{sender_domain}"
        recipient = f"contact{np.random.randint(1, 100)}@{recipient_domain}"
        subject = f"{np.random.choice(subjects)} - Ref: {np.random.randint(1000, 9999)}"
        body = np.random.choice(bodies)
        
        email_data.append({
            'id': f"SAMPLE_EMAIL_{i+1:06d}",
            'timestamp': timestamp,
            'sender': sender,
            'recipient': recipient,
            'subject': subject,
            'body': body,
            'source': 'EMAIL'
        })
    
    print(f"  Generated {len(email_data):,} sample email records")
    return email_data 

def parse_timestamp(timestamp_str):
    """Enhanced timestamp parsing with more formats"""
    if not timestamp_str:
        return None
    
    timestamp_str = str(timestamp_str).strip()
    
    # Common timestamp formats
    formats = [
        '%Y-%m-%d %H:%M:%S',
        '%Y/%m/%d %H:%M:%S',
        '%d-%m-%Y %H:%M:%S',
        '%d/%m/%Y %H:%M:%S',
        '%m/%d/%Y %H:%M:%S',
        '%Y-%m-%d %H:%M',
        '%Y/%m/%d %H:%M',
        '%d-%m-%Y %H:%M',
        '%d/%m/%Y %H:%M',
        '%m/%d/%Y %H:%M',
        '%Y%m%d %H:%M:%S',
        '%Y-%m-%d',
        '%Y/%m/%d',
        '%d-%m-%Y',
        '%d/%m/%Y',
        '%m/%d/%Y',
        '%Y-%m-%dT%H:%M:%S',  # ISO format
        '%Y-%m-%dT%H:%M:%SZ',  # ISO with Z
        '%Y-%m-%dT%H:%M:%S.%f',  # ISO with microseconds
        '%Y-%m-%dT%H:%M:%S.%fZ',  # ISO with microseconds and Z
    ]
    
    for fmt in formats:
        try:
            return datetime.strptime(timestamp_str, fmt)
        except ValueError:
            continue
    
    # Try to extract date from string using regex
    try:
        # Look for date patterns
        date_patterns = [
            r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})',  # YYYY-MM-DD or YYYY/MM/DD
            r'(\d{1,2})[-/](\d{1,2})[-/](\d{4})',  # DD-MM-YYYY or DD/MM/YYYY
            r'(\d{1,2})[-/](\d{1,2})[-/](\d{2})',  # DD-MM-YY or DD/MM/YY
        ]
        
        time_patterns = [
            r'(\d{1,2}):(\d{2}):(\d{2})',  # HH:MM:SS
            r'(\d{1,2}):(\d{2})',  # HH:MM
        ]
        
        date_match = None
        time_match = None
        
        for pattern in date_patterns:
            match = re.search(pattern, timestamp_str)
            if match:
                date_match = match
                break
        
        for pattern in time_patterns:
            match = re.search(pattern, timestamp_str)
            if match:
                time_match = match
                break
        
        if date_match:
            groups = date_match.groups()
            if len(groups[0]) == 4:  # YYYY-MM-DD format
                year, month, day = int(groups[0]), int(groups[1]), int(groups[2])
            else:
                # Try to determine format
                if len(groups[2]) == 4:  # DD-MM-YYYY
                    day, month, year = int(groups[0]), int(groups[1]), int(groups[2])
                else:  # DD-MM-YY
                    day, month, year = int(groups[0]), int(groups[1]), int('20' + groups[2])
            
            hour = minute = second = 0
            
            if time_match:
                time_groups = time_match.groups()
                if len(time_groups) == 3:
                    hour, minute, second = int(time_groups[0]), int(time_groups[1]), int(time_groups[2])
                else:
                    hour, minute = int(time_groups[0]), int(time_groups[1])
            
            return datetime(year, month, day, hour, minute, second)
    
    except Exception:
        pass
    
    return None

def data():
    """Main data loading function with comprehensive reporting"""
    print("\n" + "="*70)
    print(" LOADING FORENSIC DATA FILES")
    print("="*70)
    
    # Check what files exist
    files = os.listdir('.')
    csv_files = [f for f in files if f.lower().endswith('.csv')]
    
    print(f"\nFound {len(csv_files)} CSV files in current directory:")
    for csv_file in csv_files:
        size = os.path.getsize(csv_file)
        print(f"  • {csv_file} ({size:,} bytes)")
    
    # Load all three data sources
    print("\n" + "-"*70)
    sms_data = load_sms_data()
    print("\n" + "-"*70)
    call_data = load_call_data()
    print("\n" + "-"*70)
    email_data = load_email_data()
    
    # Summary
    total_records = len(sms_data) + len(call_data) + len(email_data)
    
    print("\n" + "="*70)
    print(" DATA LOADING SUMMARY")
    print("="*70)
    print(f" Total records loaded: {total_records:,}")
    print(f"   •  SMS Messages: {len(sms_data):,}")
    print(f"   •  Phone Calls: {len(call_data):,}")
    print(f"   •  Emails: {len(email_data):,}")
    
    if total_records == 0:
        print("\n  WARNING: No data loaded!")
        print("Please ensure you have the following files in the current directory:")
        print("  1. SMS-Data.csv")
        print("  2. CDR-Call-Details.csv")
        print("  3. emails.csv (or any CSV with email data)")
    
    return sms_data, call_data, email_data

# ============================================================================
# ANALYSIS FUNCTIONS
# ============================================================================

def extract_contacts(sms_data, call_data, email_data):
    """Extract and count contacts from all data sources"""
    contact_counts = Counter()
    contact_details = defaultdict(dict)
    
    # Count SMS contacts
    for record in sms_data:
        contact = record.get('contact', '').strip()
        if contact and contact.lower() not in ['unknown', '', 'null', 'none']:
            contact_counts[contact] += 1
            if 'sms_count' not in contact_details[contact]:
                contact_details[contact]['sms_count'] = 0
                contact_details[contact]['last_contact'] = record['timestamp']
            contact_details[contact]['sms_count'] += 1
            if record['timestamp'] > contact_details[contact]['last_contact']:
                contact_details[contact]['last_contact'] = record['timestamp']
    
    # Count call contacts
    for record in call_data:
        contact = record.get('contact', '').strip()
        if contact and contact.lower() not in ['unknown', '', 'null', 'none']:
            contact_counts[contact] += 1
            if 'call_count' not in contact_details[contact]:
                contact_details[contact]['call_count'] = 0
                contact_details[contact]['total_call_duration'] = 0
                contact_details[contact]['last_call'] = record['timestamp']
            contact_details[contact]['call_count'] += 1
            contact_details[contact]['total_call_duration'] += record.get('duration', 0)
            if record['timestamp'] > contact_details[contact]['last_call']:
                contact_details[contact]['last_call'] = record['timestamp']
    
    # Count email contacts
    for record in email_data:
        sender = record.get('sender', '').strip()
        recipient = record.get('recipient', '').strip()
        
        if sender and sender.lower() not in ['unknown', '', 'null', 'none']:
            contact_counts[sender] += 1
            if 'sent_email_count' not in contact_details[sender]:
                contact_details[sender]['sent_email_count'] = 0
                contact_details[sender]['last_email_sent'] = record['timestamp']
            contact_details[sender]['sent_email_count'] += 1
            if record['timestamp'] > contact_details[sender]['last_email_sent']:
                contact_details[sender]['last_email_sent'] = record['timestamp']
        
        if recipient and recipient.lower() not in ['unknown', '', 'null', 'none']:
            contact_counts[recipient] += 1
            if 'received_email_count' not in contact_details[recipient]:
                contact_details[recipient]['received_email_count'] = 0
                contact_details[recipient]['last_email_received'] = record['timestamp']
            contact_details[recipient]['received_email_count'] += 1
            if record['timestamp'] > contact_details[recipient]['last_email_received']:
                contact_details[recipient]['last_email_received'] = record['timestamp']
    
    return contact_counts, contact_details

def create_timeline(sms_data, call_data, email_data):
    """Create unified timeline from all data sources"""
    timeline = []
    
    print("\n Creating unified timeline...")
    
    # Add SMS events
    for record in sms_data:
        timeline.append({
            'id': record['id'],
            'timestamp': record['timestamp'],
            'contact': record.get('contact', 'Unknown'),
            'source': 'SMS',
            'type': record.get('direction', 'UNKNOWN'),
            'content': str(record.get('message', ''))[:200],
            'forensic_tag': categorize_event(record, 'SMS'),
            'details': {
                'direction': record.get('direction'),
                'message_length': len(str(record.get('message', '')))
            }
        })
    
    # Add call events
    for record in call_data:
        timeline.append({
            'id': record['id'],
            'timestamp': record['timestamp'],
            'contact': record.get('contact', 'Unknown'),
            'source': 'CALL',
            'type': record.get('type', 'UNKNOWN'),
            'content': f"Duration: {record.get('duration', 0)}s | Type: {record.get('type', '')}",
            'forensic_tag': categorize_event(record, 'CALL'),
            'details': {
                'duration': record.get('duration', 0),
                'call_type': record.get('type'),
                'churn': record.get('call_details', {}).get('churn', 'FALSE')
            }
        })
    
    # Add email events
    for record in email_data:
        timeline.append({
            'id': record['id'],
            'timestamp': record['timestamp'],
            'contact': record.get('sender', 'Unknown'),
            'source': 'EMAIL',
            'type': 'SENT',
            'content': f"To: {record.get('recipient', 'Unknown')} | Subject: {str(record.get('subject', ''))[:100]}",
            'forensic_tag': categorize_event(record, 'EMAIL'),
            'details': {
                'recipient': record.get('recipient'),
                'subject': record.get('subject'),
                'body_length': len(str(record.get('body', '')))
            }
        })
    
    # Sort by timestamp
    timeline.sort(key=lambda x: x['timestamp'])
    
    print(f"  Created timeline with {len(timeline):,} events")
    print(f"  Time range: {timeline[0]['timestamp'] if timeline else 'N/A'} to {timeline[-1]['timestamp'] if timeline else 'N/A'}")
    
    return timeline

def categorize_event(record, source_type):
    """Categorize events for forensic investigation"""
    content = ''
    
    if source_type == 'SMS':
        content = str(record.get('message', '')).lower()
    elif source_type == 'EMAIL':
        content = str(record.get('subject', '')).lower() + ' ' + str(record.get('body', '')).lower()
    elif source_type == 'CALL':
        content = str(record.get('type', '')).lower()
        # Check for specific call patterns
        if record.get('duration', 0) > 3600:  # > 1 hour
            return 'EXTENDED_COMM'
        elif record.get('call_details', {}).get('intl_mins', 0) > 5:
            return 'INTERNATIONAL'
        elif record.get('call_details', {}).get('churn', 'FALSE') == 'TRUE':
            return 'CHURN_RISK'
    
    # Forensic relevance indicators
    keywords = {
        'URGENT': ['urgent', 'emergency', 'asap', 'immediately', 'quick', 'rush', 'now'],
        'FINANCIAL': ['payment', 'bank', 'transfer', 'money', 'bitcoin', 'crypto', 'pay', 'fund', 'transaction', 'cash'],
        'SUSPICIOUS': ['delete', 'burner', 'encrypt', 'vpn', 'tor', 'secret', 'confidential', 'hide', 'cover'],
        'COORDINATION': ['meet', 'location', 'address', 'time', 'place', 'venue', 'coordinates', 'where', 'when'],
        'BUSINESS': ['meeting', 'project', 'report', 'deadline', 'client', 'customer', 'business', 'work'],
        'PERSONAL': ['love', 'dear', 'family', 'friend', 'happy', 'birthday', 'miss', 'home'],
        'SPAM': ['win', 'free', 'prize', 'offer', 'discount', 'click', 'link', 'http', 'www.']
    }
    
    for category, words in keywords.items():
        for word in words:
            if word in content:
                return category
    
    return 'ROUTINE'

def analyze_data(sms_data, call_data, email_data):
    """Enhanced analysis for forensic data"""
    print("\n" + "="*70)
    print(" FORENSIC DATA ANALYSIS")
    print("="*70)
    
    # Calculate statistics
    total_events = len(sms_data) + len(call_data) + len(email_data)
    
    if total_events == 0:
        print(" No data available for analysis.")
        return []
    
    print(f"\n DATA VOLUME ANALYSIS")
    print(f"Total Forensic Events: {total_events:,}")
    print(f"•  SMS Messages: {len(sms_data):,} ({len(sms_data)/total_events*100:.1f}%)")
    print(f"•  Phone Calls: {len(call_data):,} ({len(call_data)/total_events*100:.1f}%)")
    print(f"•  Emails: {len(email_data):,} ({len(email_data)/total_events*100:.1f}%)")
    
    # Extract contacts
    contact_counts, contact_details = extract_contacts(sms_data, call_data, email_data)
    print(f"\n CONTACT NETWORK ANALYSIS")
    print(f"Unique Contacts Found: {len(contact_counts):,}")
    
    # Show top contacts
    if contact_counts:
        top_contacts = contact_counts.most_common(15)
        print(f"\n TOP 15 MOST ACTIVE CONTACTS")
        print("-" * 80)
        print(f"{'Rank':<5} {'Contact':<35} {'Total':<8} {'SMS':<8} {'Calls':<8} {'Emails':<10}")
        print("-" * 80)
        
        for i, (contact, total_count) in enumerate(top_contacts[:15], 1):
            details = contact_details.get(contact, {})
            sms_count = details.get('sms_count', 0)
            call_count = details.get('call_count', 0)
            email_sent = details.get('sent_email_count', 0)
            email_received = details.get('received_email_count', 0)
            email_total = email_sent + email_received
            
            contact_display = contact[:32] + "..." if len(contact) > 32 else contact
            print(f"{i:<5} {contact_display:<35} {total_count:<8} {sms_count:<8} {call_count:<8} {email_total:<10}")
    
    # Create timeline
    timeline = create_timeline(sms_data, call_data, email_data)
    
    if timeline:
        # Timeline analysis
        start_date = timeline[0]['timestamp']
        end_date = timeline[-1]['timestamp']
        days_span = max((end_date - start_date).days, 1)
        
        print(f"\n TIMELINE ANALYSIS")
        print(f"Investigation Period: {days_span} days ({start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')})")
        print(f"Average daily events: {total_events/days_span:.1f}")
        print(f"Events per hour: {total_events/(days_span*24):.1f}")
        
        # Hourly patterns
        print(f"\n TEMPORAL PATTERNS")
        hourly_counts = {hour: 0 for hour in range(24)}
        weekday_counts = {day: 0 for day in range(7)}
        
        for event in timeline:
            hour = event['timestamp'].hour
            weekday = event['timestamp'].weekday()
            hourly_counts[hour] += 1
            weekday_counts[weekday] += 1
        
        peak_hour = max(hourly_counts, key=hourly_counts.get)
        peak_day = max(weekday_counts, key=weekday_counts.get)
        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        
        print(f"Peak Activity Hour: {peak_hour:02d}:00 ({hourly_counts[peak_hour]:,} events)")
        print(f"Busiest Day: {day_names[peak_day]} ({weekday_counts[peak_day]:,} events)")
        
        # Communication patterns by time of day
        morning = sum(hourly_counts[h] for h in range(6, 12))
        afternoon = sum(hourly_counts[h] for h in range(12, 18))
        evening = sum(hourly_counts[h] for h in range(18, 24))
        night = sum(hourly_counts[h] for h in range(0, 6))
        
        print(f"\n COMMUNICATION BY TIME OF DAY")
        print(f"Morning (6AM-12PM): {morning:,} events ({morning/total_events*100:.1f}%)")
        print(f"Afternoon (12PM-6PM): {afternoon:,} events ({afternoon/total_events*100:.1f}%)")
        print(f"Evening (6PM-12AM): {evening:,} events ({evening/total_events*100:.1f}%)")
        print(f"Night (12AM-6AM): {night:,} events ({night/total_events*100:.1f}%)")
        
        # Forensic categories
        print(f"\n FORENSIC CATEGORY ANALYSIS")
        categories = Counter(event['forensic_tag'] for event in timeline)
        total_categorized = len(timeline)
        
        print(f"{'Category':<15} {'Count':>10} {'Percentage':>12}")
        print("-" * 40)
        for category, count in categories.most_common():
            percentage = (count / total_categorized) * 100
            print(f"{category:<15} {count:>10,} {percentage:>11.1f}%")
        
        # Source distribution
        print(f"\n SOURCE DISTRIBUTION")
        sources = Counter(event['source'] for event in timeline)
        for source, count in sources.most_common():
            percentage = (count / total_categorized) * 100
            print(f"{source:<10} {count:>10,} {percentage:>11.1f}%")
        
        # Suspicious patterns
        print(f"\n RISK ASSESSMENT & RED FLAGS")
        flags = detect_suspicious_patterns(timeline)
        
        if flags:
            risk_score = min(len(flags) * 10, 100)
            print(f"Overall Risk Score: {risk_score}/100")
            print("\nDetected Issues:")
            for flag in flags:
                print(f"  • {flag}")
        else:
            print("No significant red flags detected.")
            print("Risk Score: 0/100 (Low Risk)")
    
    return timeline

def detect_suspicious_patterns(timeline):
    """Detect potentially suspicious patterns"""
    flags = []
    
    if not timeline or len(timeline) < 10:
        return flags
    
    total_events = len(timeline)
    
    # 1. Late-night communications (midnight to 5 AM)
    late_night = [e for e in timeline if 0 <= e['timestamp'].hour <= 5]
    late_night_percentage = len(late_night) / total_events * 100
    if late_night_percentage > 20:  # More than 20% at night
        flags.append(f"High late-night activity: {len(late_night):,} events ({late_night_percentage:.1f}%)")
    
    # 2. Rapid communications (multiple events within minutes)
    timeline.sort(key=lambda x: x['timestamp'])
    rapid_sequences = 0
    for i in range(1, len(timeline)):
        time_diff = (timeline[i]['timestamp'] - timeline[i-1]['timestamp']).seconds
        if time_diff < 30:  # Less than 30 seconds
            rapid_sequences += 1
    
    if rapid_sequences > total_events * 0.05:  # More than 5%
        flags.append(f"Rapid-fire communications: {rapid_sequences:,} sequences <30s apart")
    
    # 3. Unknown contacts
    unknown_contacts = sum(1 for e in timeline if 'unknown' in str(e.get('contact', '')).lower())
    if unknown_contacts > total_events * 0.1:  # More than 10%
        flags.append(f"High unknown contacts: {unknown_contacts:,} ({unknown_contacts/total_events*100:.1f}%)")
    
    # 4. Financial keywords
    financial_events = sum(1 for e in timeline if e.get('forensic_tag') == 'FINANCIAL')
    if financial_events > 10:
        flags.append(f"Financial-related communications: {financial_events:,}")
    
    # 5. Suspicious keywords
    suspicious_events = sum(1 for e in timeline if e.get('forensic_tag') == 'SUSPICIOUS')
    if suspicious_events > 5:
        flags.append(f"Suspicious keyword communications: {suspicious_events:,}")
    
    # 6. International calls
    international_calls = sum(1 for e in timeline if e.get('forensic_tag') == 'INTERNATIONAL')
    if international_calls > 3:
        flags.append(f"International communications: {international_calls:,}")
    
    # 7. Extended communications
    extended_comms = sum(1 for e in timeline if e.get('forensic_tag') == 'EXTENDED_COMM')
    if extended_comms > 2:
        flags.append(f"Extended communications (>1 hour): {extended_comms:,}")
    
    # 8. Activity on weekends
    weekend_events = sum(1 for e in timeline if e['timestamp'].weekday() >= 5)  # 5=Sat, 6=Sun
    weekend_percentage = weekend_events / total_events * 100
    if weekend_percentage > 40:  # More than 40% on weekends
        flags.append(f"High weekend activity: {weekend_percentage:.1f}%")
    
    return flags[:10]  # Return top 10 flags

# ============================================================================
# EXPORT FUNCTIONS
# ============================================================================

def export_forensic_report(timeline, sms_data, call_data, email_data):
    """Export comprehensive forensic report"""
    print("\n" + "="*70)
    print(" EXPORTING FORENSIC REPORT")
    print("="*70)
    
    if not timeline:
        print(" No data to export.")
        return
    
    report_content = []
    report_content.append("=" * 80)
    report_content.append("DIGITAL FORENSIC INVESTIGATION REPORT")
    report_content.append("=" * 80)
    report_content.append(f"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_content.append(f"Case Reference: DF-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
    report_content.append("")
    
    # Executive Summary
    report_content.append("EXECUTIVE SUMMARY")
    report_content.append("-" * 40)
    total_events = len(sms_data) + len(call_data) + len(email_data)
    report_content.append(f"Total Events Analyzed: {total_events:,}")
    
    if timeline:
        days_span = max((timeline[-1]['timestamp'] - timeline[0]['timestamp']).days, 1)
        report_content.append(f"Analysis Period: {days_span} days")
        report_content.append(f"Date Range: {timeline[0]['timestamp'].strftime('%Y-%m-%d')} to {timeline[-1]['timestamp'].strftime('%Y-%m-%d')}")
        report_content.append(f"Average Daily Events: {total_events/days_span:.1f}")
    
    # Data Sources
    report_content.append("")
    report_content.append("DATA SOURCES")
    report_content.append("-" * 40)
    report_content.append(f"SMS Messages: {len(sms_data):,}")
    report_content.append(f"Phone Calls: {len(call_data):,}")
    report_content.append(f"Emails: {len(email_data):,}")
    
    # Contact Analysis
    contact_counts, contact_details = extract_contacts(sms_data, call_data, email_data)
    report_content.append("")
    report_content.append("CONTACT ANALYSIS")
    report_content.append("-" * 40)
    report_content.append(f"Unique Contacts Identified: {len(contact_counts):,}")
    
    if contact_counts:
        top_contacts = contact_counts.most_common(20)
        report_content.append("")
        report_content.append("TOP 20 CONTACTS BY INTERACTION VOLUME:")
        report_content.append("-" * 80)
        report_content.append(f"{'Rank':<5} {'Contact':<40} {'Total':<8} {'SMS':<8} {'Calls':<8} {'Emails':<10}")
        report_content.append("-" * 80)
        
        for i, (contact, total_count) in enumerate(top_contacts, 1):
            details = contact_details.get(contact, {})
            sms_count = details.get('sms_count', 0)
            call_count = details.get('call_count', 0)
            email_sent = details.get('sent_email_count', 0)
            email_received = details.get('received_email_count', 0)
            email_total = email_sent + email_received
            
            contact_display = contact[:38] + "..." if len(contact) > 38 else contact
            report_content.append(f"{i:<5} {contact_display:<40} {total_count:<8} {sms_count:<8} {call_count:<8} {email_total:<10}")
    
    # Timeline Summary
    if timeline:
        report_content.append("")
        report_content.append("TIMELINE SUMMARY")
        report_content.append("-" * 40)
        
        # Group by date
        date_groups = {}
        for event in timeline:
            date = event['timestamp'].strftime('%Y-%m-%d')
            if date not in date_groups:
                date_groups[date] = []
            date_groups[date].append(event)
        
        # Busiest days
        busy_days = sorted(date_groups.items(), key=lambda x: len(x[1]), reverse=True)[:10]
        report_content.append("TOP 10 MOST ACTIVE DAYS:")
        for date, events in busy_days:
            report_content.append(f"  • {date}: {len(events):,} events")
    
    # Forensic Findings
    if timeline:
        report_content.append("")
        report_content.append("FORENSIC FINDINGS")
        report_content.append("-" * 40)
        
        categories = Counter(event['forensic_tag'] for event in timeline)
        total_categorized = len(timeline)
        
        report_content.append("EVENT CATEGORIZATION:")
        for category, count in categories.most_common():
            percentage = (count / total_categorized) * 100
            report_content.append(f"  • {category}: {count:,} events ({percentage:.1f}%)")
        
        flags = detect_suspicious_patterns(timeline)
        if flags:
            report_content.append("")
            report_content.append("POTENTIAL RED FLAGS / ANOMALIES:")
            for flag in flags:
                report_content.append(f"  ⚠ {flag}")
        
        # Risk Assessment
        risk_score = min(len(flags) * 10, 100)
        report_content.append("")
        report_content.append("RISK ASSESSMENT:")
        report_content.append(f"  Overall Risk Score: {risk_score}/100")
        if risk_score < 30:
            report_content.append("  Risk Level: LOW")
        elif risk_score < 70:
            report_content.append("  Risk Level: MEDIUM")
        else:
            report_content.append("  Risk Level: HIGH")
    
    # Recommendations
    report_content.append("")
    report_content.append("INVESTIGATIVE RECOMMENDATIONS")
    report_content.append("-" * 40)
    
    flags = detect_suspicious_patterns(timeline)
    if flags:
        report_content.append("1. Further investigation recommended for identified anomalies")
        report_content.append("2. Review communications with top 20 contacts")
        report_content.append("3. Analyze late-night and rapid-fire communications")
        if any('financial' in flag.lower() for flag in flags):
            report_content.append("4. Scrutinize financial-related communications")
        if any('international' in flag.lower() for flag in flags):
            report_content.append("5. Review international communications")
    else:
        report_content.append("No significant anomalies detected. Standard monitoring recommended.")
    
    # Save report
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_filename = f'forensic_report_{timestamp}.txt'
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_content))
    
    print(f" Comprehensive forensic report saved to '{report_filename}'")
    
    # Export additional files
    export_timeline_csv(timeline)
    export_contacts_csv(contact_counts, contact_details)
    export_summary_json(timeline, sms_data, call_data, email_data, contact_counts, contact_details)

def export_timeline_csv(timeline):
    """Export timeline to CSV"""
    if not timeline:
        return
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'forensic_timeline_{timestamp}.csv'
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['timestamp', 'id', 'source', 'contact', 'type', 'content', 'forensic_tag']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for event in timeline:
            row = event.copy()
            row['timestamp'] = event['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            # Remove details field if present
            if 'details' in row:
                del row['details']
            writer.writerow(row)
    
    print(f" Detailed timeline saved to '{filename}'")

def export_contacts_csv(contact_counts, contact_details):
    """Export contacts to CSV"""
    if not contact_counts:
        return
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'forensic_contacts_{timestamp}.csv'
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Contact', 'Total_Interactions', 'SMS_Count', 'Call_Count', 
                        'Email_Sent_Count', 'Email_Received_Count', 'Last_Contact_Date'])
        
        for contact, total_count in contact_counts.most_common():
            details = contact_details.get(contact, {})
            sms_count = details.get('sms_count', 0)
            call_count = details.get('call_count', 0)
            email_sent = details.get('sent_email_count', 0)
            email_received = details.get('received_email_count', 0)
            
            # Get last contact date
            last_dates = []
            if 'last_contact' in details:
                last_dates.append(details['last_contact'])
            if 'last_call' in details:
                last_dates.append(details['last_call'])
            if 'last_email_sent' in details:
                last_dates.append(details['last_email_sent'])
            if 'last_email_received' in details:
                last_dates.append(details['last_email_received'])
            
            last_contact = max(last_dates) if last_dates else 'Unknown'
            if last_contact != 'Unknown':
                last_contact = last_contact.strftime('%Y-%m-%d %H:%M:%S')
            
            writer.writerow([contact, total_count, sms_count, call_count, 
                           email_sent, email_received, last_contact])
    
    print(f" Contact analysis saved to '{filename}'")

def export_summary_json(timeline, sms_data, call_data, email_data, contact_counts, contact_details):
    """Export summary data to JSON"""
    if not timeline:
        return
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'forensic_summary_{timestamp}.json'
    
    summary = {
        'report_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_summary': {
            'total_events': len(sms_data) + len(call_data) + len(email_data),
            'sms_count': len(sms_data),
            'call_count': len(call_data),
            'email_count': len(email_data)
        },
        'timeline_summary': {
            'start_date': timeline[0]['timestamp'].strftime('%Y-%m-%d %H:%M:%S') if timeline else None,
            'end_date': timeline[-1]['timestamp'].strftime('%Y-%m-%d %H:%M:%S') if timeline else None,
            'total_events': len(timeline)
        },
        'contact_summary': {
            'unique_contacts': len(contact_counts),
            'top_contacts': dict(contact_counts.most_common(10))
        },
        'risk_assessment': {
            'flags': detect_suspicious_patterns(timeline),
            'risk_score': min(len(detect_suspicious_patterns(timeline)) * 10, 100)
        }
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, default=str)
    
    print(f" Summary data saved to '{filename}'")

# ============================================================================
# VISUALIZATION FUNCTIONS
# ============================================================================

def visualize_data(sms_data, call_data, email_data):
    """Visualize forensic data"""
    try:
        import matplotlib.pyplot as plt
        import matplotlib
        # Try different backends
        for backend in ['TkAgg', 'Qt5Agg', 'Agg']:
            try:
                matplotlib.use(backend)
                break
            except:
                continue
    except ImportError:
        print(" Matplotlib not installed. Install with: pip install matplotlib")
        return
    
    print("\n" + "="*70)
    print(" FORENSIC DATA VISUALIZATION")
    print("="*70)
    
    timeline = create_timeline(sms_data, call_data, email_data)
    
    if not timeline:
        print(" No data available for visualization.")
        return
    
    try:
        # Create comprehensive visualization
        fig = plt.figure(figsize=(20, 15))
        fig.suptitle('Digital Forensic Analysis Dashboard', fontsize=16, fontweight='bold')
        
        # Convert to DataFrame
        df = pd.DataFrame(timeline)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        df['date'] = df['timestamp'].dt.date
        df['hour'] = df['timestamp'].dt.hour
        df['day_of_week'] = df['timestamp'].dt.day_name()
        df['month'] = df['timestamp'].dt.to_period('M').astype(str)
        
        # 1. Daily Activity (Top left)
        ax1 = plt.subplot(3, 3, 1)
        daily_counts = df.groupby('date').size()
        ax1.plot(daily_counts.index, daily_counts.values, marker='o', linewidth=2, color='#2E86AB')
        ax1.set_title('Daily Communication Volume', fontsize=12, fontweight='bold')
        ax1.set_xlabel('Date')
        ax1.set_ylabel('Number of Events')
        ax1.tick_params(axis='x', rotation=45)
        ax1.grid(True, alpha=0.3)
        
        # 2. Source Distribution (Top middle)
        ax2 = plt.subplot(3, 3, 2)
        source_counts = df['source'].value_counts()
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        ax2.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%',
                colors=colors, startangle=90, textprops={'fontsize': 10})
        ax2.set_title('Communication Source Distribution', fontsize=12, fontweight='bold')
        
        # 3. Hourly Pattern (Top right)
        ax3 = plt.subplot(3, 3, 3)
        hourly_counts = df['hour'].value_counts().sort_index()
        ax3.bar(hourly_counts.index, hourly_counts.values, color='#96CEB4', alpha=0.7)
        ax3.set_title('Hourly Activity Pattern', fontsize=12, fontweight='bold')
        ax3.set_xlabel('Hour of Day')
        ax3.set_ylabel('Number of Events')
        ax3.set_xticks(range(0, 24, 3))
        ax3.grid(True, alpha=0.3)
        
        # 4. Forensic Categories (Middle left)
        ax4 = plt.subplot(3, 3, 4)
        if 'forensic_tag' in df.columns:
            tag_counts = df['forensic_tag'].value_counts()
            colors = ['#FF6B6B', '#FFD166', '#06D6A0', '#118AB2', '#073B4C', '#EF476F'][:len(tag_counts)]
            bars = ax4.bar(range(len(tag_counts)), tag_counts.values, color=colors, alpha=0.7)
            ax4.set_title('Forensic Category Distribution', fontsize=12, fontweight='bold')
            ax4.set_xlabel('Category')
            ax4.set_ylabel('Count')
            ax4.set_xticks(range(len(tag_counts)))
            ax4.set_xticklabels(tag_counts.index, rotation=45, ha='right')
            ax4.grid(True, alpha=0.3)
            
            # Add value labels on bars
            for bar in bars:
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                        f'{int(height):,}', ha='center', va='bottom', fontsize=8)
        
        # 5. Day of Week Pattern (Middle)
        ax5 = plt.subplot(3, 3, 5)
        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        day_counts = df['day_of_week'].value_counts().reindex(day_order)
        ax5.bar(range(len(day_counts)), day_counts.values, color='#F4A261', alpha=0.7)
        ax5.set_title('Activity by Day of Week', fontsize=12, fontweight='bold')
        ax5.set_xlabel('Day of Week')
        ax5.set_ylabel('Number of Events')
        ax5.set_xticks(range(len(day_counts)))
        ax5.set_xticklabels(day_counts.index, rotation=45, ha='right')
        ax5.grid(True, alpha=0.3)
        
        # 6. Monthly Trend (Middle right)
        ax6 = plt.subplot(3, 3, 6)
        monthly_counts = df.groupby('month').size()
        ax6.plot(range(len(monthly_counts)), monthly_counts.values, 
                marker='s', linewidth=2, color='#9D4EDD')
        ax6.set_title('Monthly Communication Trend', fontsize=12, fontweight='bold')
        ax6.set_xlabel('Month')
        ax6.set_ylabel('Number of Events')
        ax6.set_xticks(range(len(monthly_counts)))
        ax6.set_xticklabels(monthly_counts.index, rotation=45, ha='right')
        ax6.grid(True, alpha=0.3)
        
        # 7. Contact Network (Bottom left - large)
        ax7 = plt.subplot(3, 2, 5)
        contact_counts, _ = extract_contacts(sms_data, call_data, email_data)
        if contact_counts:
            top_contacts = contact_counts.most_common(15)
            contact_names = [c[0][:20] + '...' if len(c[0]) > 20 else c[0] for c in top_contacts]
            contact_values = [c[1] for c in top_contacts]
            
            bars = ax7.barh(range(len(contact_names)), contact_values, color='#E63946', alpha=0.7)
            ax7.set_title('Top 15 Contacts by Interaction Count', fontsize=12, fontweight='bold')
            ax7.set_xlabel('Number of Interactions')
            ax7.set_yticks(range(len(contact_names)))
            ax7.set_yticklabels(contact_names)
            ax7.invert_yaxis()  # Highest at top
            ax7.grid(True, alpha=0.3, axis='x')
            
            # Add value labels
            for i, (bar, value) in enumerate(zip(bars, contact_values)):
                ax7.text(value + max(contact_values)*0.01, bar.get_y() + bar.get_height()/2.,
                        f'{value:,}', ha='left', va='center', fontsize=8)
        
        # 8. Communication Type Distribution (Bottom right)
        ax8 = plt.subplot(3, 2, 6)
        type_data = []
        type_labels = []
        
        # Get call types
        call_types = Counter()
        for record in call_data:
            call_types[record.get('type', 'UNKNOWN')] += 1
        
        for call_type, count in call_types.most_common(5):
            type_data.append(count)
            type_labels.append(f'Call: {call_type}')
        
        # Get SMS directions
        sms_directions = Counter()
        for record in sms_data:
            sms_directions[record.get('direction', 'UNKNOWN')] += 1
        
        for direction, count in sms_directions.most_common(2):
            type_data.append(count)
            type_labels.append(f'SMS: {direction}')
        
        if type_data:
            colors = plt.cm.Set3(np.linspace(0, 1, len(type_data)))
            wedges, texts, autotexts = ax8.pie(type_data, labels=type_labels, autopct='%1.1f%%',
                                              colors=colors, startangle=90, textprops={'fontsize': 9})
            ax8.set_title('Communication Type Breakdown', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'forensic_dashboard_{timestamp}.png'
        plt.savefig(filename, dpi=150, bbox_inches='tight')
        plt.show()
        
        print(f"\n Forensic visualization saved to '{filename}'")
        
    except Exception as e:
        print(f" Visualization error: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    """Main function with enhanced error handling"""
    print("\n" + "="*70)
    print(" DIGITAL FORENSIC CORRELATION SYSTEM")
    print("="*70)
    print("\nLoading forensic data...")
    
    try:
        # Load data
        sms_data, call_data, email_data = data()
        
        if not sms_data and not call_data and not email_data:
            print("\n No data available. Exiting.")
            return
        
        # Create timeline
        print("\n" + "="*70)
        print(" CREATING UNIFIED TIMELINE")
        print("="*70)
        timeline = create_timeline(sms_data, call_data, email_data)
        
        if not timeline:
            print(" Failed to create timeline. Exiting.")
            return
        
        # Main menu loop
        while True:
            print("\n" + "="*70)
            print(" FORENSIC ANALYST MENU")
            print("="*70)
            print("1.  Search Timeline Events")
            print("2.  View Detailed Analysis")
            print("3.  Export Forensic Report")
            print("4.  Visualize Data Patterns")
            print("5.  Export All Data for Legal Proceedings")
            print("6.  Exit")
            print("="*70)
            
            choice = input("\nSelect option (1-6): ").strip()
            
            if choice == '1':
                # Search functionality
                print("\n" + "="*70)
                print(" SEARCH TIMELINE EVENTS")
                print("="*70)
                search_term = input("Enter search term (contact/phone/email/keyword): ").strip().lower()
                
                if search_term:
                    results = []
                    for event in timeline:
                        if (search_term in str(event.get('contact', '')).lower() or
                            search_term in str(event.get('content', '')).lower() or
                            search_term in str(event.get('source', '')).lower() or
                            search_term in str(event.get('type', '')).lower() or
                            search_term in str(event.get('forensic_tag', '')).lower()):
                            results.append(event)
                    
                    if results:
                        print(f"\n Found {len(results):,} matching events:")
                        print("-" * 100)
                        print(f"{'Timestamp':<20} {'Source':<8} {'Contact':<25} {'Type':<12} {'Content':<30}")
                        print("-" * 100)
                        
                        for event in results[:20]:  # Show first 20
                            timestamp = event['timestamp'].strftime('%Y-%m-%d %H:%M')
                            source = event['source']
                            contact = event['contact'][:23] + "..." if len(event['contact']) > 23 else event['contact']
                            etype = event['type'][:10] if event['type'] else ''
                            content = event['content'][:27] + "..." if len(event['content']) > 27 else event['content']
                            
                            print(f"{timestamp:<20} {source:<8} {contact:<25} {etype:<12} {content:<30}")
                        
                        if len(results) > 20:
                            print(f"... and {len(results) - 20:,} more results")
                        
                        # Export search results
                        export_search = input("\nExport these results to CSV? (y/n): ").lower()
                        if export_search == 'y':
                            export_search_results(results, search_term)
                    else:
                        print(" No matching events found.")
            
            elif choice == '2':
                analyze_data(sms_data, call_data, email_data)
            
            elif choice == '3':
                export_forensic_report(timeline, sms_data, call_data, email_data)
            
            elif choice == '4':
                visualize_data(sms_data, call_data, email_data)
            
            elif choice == '5':
                print("\n" + "="*70)
                print(" EXPORTING ALL DATA FOR LEGAL PROCEEDINGS")
                print("="*70)
                export_forensic_report(timeline, sms_data, call_data, email_data)
                print("\n All forensic data exported for legal proceedings:")
                
                # List exported files
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                exported_files = [
                    f'forensic_report_{timestamp}.txt',
                    f'forensic_timeline_{timestamp}.csv',
                    f'forensic_contacts_{timestamp}.csv',
                    f'forensic_summary_{timestamp}.json',
                    f'forensic_dashboard_{timestamp}.png'
                ]
                
                for file in exported_files:
                    if os.path.exists(file):
                        size = os.path.getsize(file)
                        print(f"  • {file} ({size:,} bytes)")
            
            elif choice == '6':
                print("\n" + "="*70)
                print(" INVESTIGATION SESSION COMPLETED")
                print("="*70)
                print("All reports and data have been saved.")
                print("Thank you for using the Digital Forensic Correlation System.")
                break
            
            else:
                print(" Invalid option. Please try again.")
    
    except KeyboardInterrupt:
        print("\n\n  Investigation interrupted by user.")
    except Exception as e:
        print(f"\n Unexpected error: {e}")
        import traceback
        traceback.print_exc()

def export_search_results(results, search_term):
    """Export search results to CSV"""
    if not results:
        return
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'search_results_{search_term}_{timestamp}.csv'
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['timestamp', 'id', 'source', 'contact', 'type', 'content', 'forensic_tag']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for event in results:
            row = event.copy()
            row['timestamp'] = event['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            if 'details' in row:
                del row['details']
            writer.writerow(row)
    
    print(f" Search results exported to '{filename}'")

if __name__ == "__main__":
    main()
