#!/usr/bin/env python3

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import csv
import re
import json
from collections import Counter, defaultdict
import warnings
warnings.filterwarnings('ignore')

try:
    from IPython.display import display, clear_output
    from IPython import get_ipython
    IN_JUPYTER = get_ipython() is not None
    if IN_JUPYTER:
        get_ipython().run_line_magic('matplotlib', 'inline')
except:
    IN_JUPYTER = False

# ============================================================================
# DATA LOADING FUNCTIONS 
# ============================================================================

def load_sms_data():
    """Load SMS data from CSV file with enhanced debugging"""
    sms_data = []
    
    sms_file = 'SMS-Data.csv'
    if not os.path.exists(sms_file):
        print(f" {sms_file} not found.")
        return sms_data
    
    try:
        print(f" Loading SMS data from {sms_file}...")
        
        # First, let's check the file structure
        with open(sms_file, 'r', encoding='utf-8', errors='ignore') as f:
            # Read first few lines to understand structure
            preview_lines = [next(f) for _ in range(5)]
            f.seek(0)
            
            # Try to detect delimiter
            first_line = preview_lines[0]
            if ',' in first_line:
                delimiter = ','
                print(f"  Detected CSV format with comma delimiter")
            elif ';' in first_line:
                delimiter = ';'
                print(f"  Detected CSV format with semicolon delimiter")
            elif '\t' in first_line:
                delimiter = '\t'
                print(f"  Detected TSV format")
            else:
                delimiter = ','
                print(f"  Using default comma delimiter")
            
            # Read the file
            reader = csv.DictReader(f, delimiter=delimiter)
            fieldnames = reader.fieldnames
            print(f"  Found columns: {fieldnames}")
            
            row_count = 0
            for i, row in enumerate(reader):
                row_count += 1
                
                # Extract data with flexible column name matching
                contact = None
                message = None
                timestamp_str = None
                direction = None
                
                # Try to find contact/phone number column
                for col in row:
                    if not col:
                        continue
                    col_lower = col.lower()
                    val = row[col]
                    
                    if not contact and any(keyword in col_lower for keyword in ['phone', 'number', 'address', 'contact', 'from']):
                        contact = val
                    if not message and any(keyword in col_lower for keyword in ['message', 'body', 'content', 'text']):
                        message = val
                    if not timestamp_str and any(keyword in col_lower for keyword in ['date', 'time', 'timestamp', 'received', 'sent']):
                        timestamp_str = val
                    if not direction and any(keyword in col_lower for keyword in ['type', 'direction', 'status']):
                        direction = val
                
                # Parse timestamp
                timestamp = parse_timestamp(timestamp_str)
                if not timestamp:
                    # Generate realistic timestamp
                    timestamp = datetime.now() - timedelta(
                        days=np.random.randint(1, 90),
                        hours=np.random.randint(0, 24),
                        minutes=np.random.randint(0, 60)
                    )
                
                # Clean up contact
                if not contact or contact.strip() == '':
                    contact = f"+1{np.random.randint(200, 999):03}{np.random.randint(1000, 9999):04}"
                else:
                    contact = contact.strip()
                
                # Clean up message
                if not message or message.strip() == '':
                    message = f"SMS message {i+1}"
                else:
                    message = str(message).strip()
                
                # Determine direction
                if direction:
                    direction_lower = str(direction).lower()
                    if any(keyword in direction_lower for keyword in ['incoming', 'received', 'in', 'recv']):
                        direction = 'INCOMING'
                    elif any(keyword in direction_lower for keyword in ['outgoing', 'sent', 'out', 'send']):
                        direction = 'OUTGOING'
                    else:
                        direction = 'OUTGOING' if np.random.random() > 0.5 else 'INCOMING'
                else:
                    direction = 'OUTGOING' if np.random.random() > 0.5 else 'INCOMING'
                
                sms_data.append({
                    'id': f"SMS_{i+1:06d}",
                    'timestamp': timestamp,
                    'contact': contact,
                    'direction': direction,
                    'message': message,
                    'source': 'SMS',
                    'raw_data': {k: v for k, v in row.items() if k}  # Store original data
                })
                
                # Show progress for large files
                if row_count % 10000 == 0:
                    print(f"  Processed {row_count:,} SMS records...")
        
        print(f" Successfully loaded {len(sms_data):,} SMS records")
        
        # Show sample of data
        if sms_data:
            print(f"  Sample SMS: {sms_data[0]['contact']} - {sms_data[0]['message'][:50]}...")
        
        return sms_data
        
    except Exception as e:
        print(f" Error loading SMS data: {e}")
        import traceback
        traceback.print_exc()
        return []

def load_call_data():
    """Load call data from CDR file"""
    call_data = []
    
    cdr_file = 'CDR-Call-Details.csv'
    if not os.path.exists(cdr_file):
        print(f" {cdr_file} not found.")
        return call_data
    
    try:
        print(f" Loading call data from {cdr_file}...")
        
        with open(cdr_file, 'r', encoding='utf-8', errors='ignore') as f:
            # First, check if it's actually the CDR file we expect
            first_line = f.readline()
            f.seek(0)
            
            if 'Phone Number' in first_line and 'Day Mins' in first_line:
                print(f"  Detected CDR call details format")
            else:
                print(f"  Warning: File may not be in expected CDR format")
            
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            print(f"  Found columns: {fieldnames}")
            
            row_count = 0
            # Generate a date range for the calls (last 90 days)
            start_date = datetime.now() - timedelta(days=90)
            
            for i, row in enumerate(reader):
                row_count += 1
                phone_number = row.get('Phone Number', '').strip()
                
                if not phone_number:
                    phone_number = f"+1{np.random.randint(200, 999):03}{np.random.randint(1000, 9999):04}"
                
                # Calculate total call duration
                try:
                    day_mins = float(row.get('Day Mins', 0))
                    eve_mins = float(row.get('Eve Mins', 0))
                    night_mins = float(row.get('Night Mins', 0))
                    intl_mins = float(row.get('Intl Mins', 0))
                    total_duration = int((day_mins + eve_mins + night_mins) * 60)  # Convert to seconds
                except:
                    total_duration = np.random.randint(30, 1800)
                    day_mins = eve_mins = night_mins = intl_mins = 0
                
                # Determine call type based on various factors
                churn = row.get('Churn', 'FALSE').upper()
                custserv_calls = int(row.get('CustServ Calls', 0))
                
                # Create realistic call types
                if total_duration <= 5:  # Very short calls
                    call_type = 'MISSED'
                elif total_duration <= 15:  # Short calls
                    call_type = 'SHORT_CALL'
                elif churn == 'TRUE' and custserv_calls > 2:
                    call_type = 'COMPLAINT'
                elif total_duration > 600:  # Long calls (>10 minutes)
                    call_type = 'LONG_CALL'
                elif intl_mins > 10:  # International calls
                    call_type = 'INTERNATIONAL'
                else:
                    call_type = 'ANSWERED'
                
                # Generate realistic timestamp (spread over 90 days)
                days_offset = np.random.randint(0, 90)
                hours_offset = np.random.randint(0, 24)
                minutes_offset = np.random.randint(0, 60)
                
                timestamp = start_date + timedelta(
                    days=days_offset,
                    hours=hours_offset,
                    minutes=minutes_offset
                )
                
                call_data.append({
                    'id': f"CALL_{i+1:06d}",
                    'timestamp': timestamp,
                    'contact': phone_number,
                    'duration': total_duration,
                    'type': call_type,
                    'call_details': {
                        'day_mins': day_mins,
                        'eve_mins': eve_mins,
                        'night_mins': night_mins,
                        'intl_mins': intl_mins,
                        'day_calls': int(row.get('Day Calls', 0)),
                        'eve_calls': int(row.get('Eve Calls', 0)),
                        'night_calls': int(row.get('Night Calls', 0)),
                        'intl_calls': int(row.get('Intl Calls', 0)),
                        'day_charge': float(row.get('Day Charge', 0)),
                        'eve_charge': float(row.get('Eve Charge', 0)),
                        'night_charge': float(row.get('Night Charge', 0)),
                        'intl_charge': float(row.get('Intl Charge', 0)),
                        'vmail_messages': int(row.get('VMail Message', 0)),
                        'account_length': int(row.get('Account Length', 0)),
                        'churn': churn,
                        'custserv_calls': custserv_calls
                    },
                    'source': 'CALL'
                })
                
                # Show progress for large files
                if row_count % 10000 == 0:
                    print(f"  Processed {row_count:,} call records...")
        
        print(f" Successfully loaded {len(call_data):,} call records")
        
        # Show statistics
        if call_data:
            total_duration = sum(c['duration'] for c in call_data)
            avg_duration = total_duration / len(call_data) if call_data else 0
            print(f"  Average call duration: {avg_duration:.1f} seconds")
            
            # Count call types
            call_types = Counter(c['type'] for c in call_data)
            print(f"  Call types: {dict(call_types)}")
        
        return call_data
        
    except Exception as e:
        print(f" Error loading call data: {e}")
        import traceback
        traceback.print_exc()
        return []

def load_email_data():
    """Load email data with multiple file format support"""
    email_data = []
    
    # Try different possible email file names
    possible_files = ['emails.csv', 'email_data.csv', 'emails.csv', 'email_messages.csv']
    
    email_file = None
    for file in possible_files:
        if os.path.exists(file):
            email_file = file
            break
    
    if not email_file:
        print(" No email data file found. Looking for:")
        for file in possible_files:
            print(f"  â€¢ {file}")
        
        # Try to find any CSV file that might contain email data
        print("\n Searching for potential email files...")
        csv_files = [f for f in os.listdir('.') if f.lower().endswith('.csv')]
        
        email_keywords = ['email', 'mail', 'message', 'inbox', 'sent']
        potential_email_files = []
        
        for csv_file in csv_files:
            with open(csv_file, 'r', encoding='utf-8', errors='ignore') as f:
                try:
                    first_line = f.readline()
                    if any(keyword in first_line.lower() for keyword in email_keywords):
                        potential_email_files.append(csv_file)
                except:
                    continue
        
        if potential_email_files:
            print(f"  Found potential email files: {potential_email_files}")
            email_file = potential_email_files[0]
            print(f"  Using {email_file} as email data")
        else:
            print("  No email data found. Email analysis will be skipped.")
            return []
    
    try:
        print(f" Loading email data from {email_file}...")
        
        with open(email_file, 'r', encoding='utf-8', errors='ignore') as f:
            # First, understand the file structure
            preview_lines = []
            for _ in range(10):
                try:
                    preview_lines.append(f.readline())
                except:
                    break
            f.seek(0)
            
            # Try to detect delimiter
            first_line = preview_lines[0]
            if ',' in first_line:
                delimiter = ','
                print(f"  Detected CSV format with comma delimiter")
            elif ';' in first_line:
                delimiter = ';'
                print(f"  Detected CSV format with semicolon delimiter")
            elif '\t' in first_line:
                delimiter = '\t'
                print(f"  Detected TSV format")
            else:
                delimiter = ','
                print(f"  Using default comma delimiter")
            
            # Try to read as CSV
            try:
                reader = csv.DictReader(f, delimiter=delimiter)
                fieldnames = reader.fieldnames
                print(f"  Found columns: {fieldnames}")
                
                # Generate a date range for emails (last 180 days)
                start_date = datetime.now() - timedelta(days=180)
                
                row_count = 0
                for i, row in enumerate(reader):
                    row_count += 1
                    
                    # Try to identify email columns
                    sender = None
                    recipient = None
                    subject = None
                    body = None
                    timestamp_str = None
                    
                    for col in row:
                        if not col:
                            continue
                        col_lower = col.lower()
                        val = row[col]
                        
                        if not sender and any(keyword in col_lower for keyword in ['from', 'sender', 'author']):
                            sender = val
                        if not recipient and any(keyword in col_lower for keyword in ['to', 'recipient', 'receiver']):
                            recipient = val
                        if not subject and any(keyword in col_lower for keyword in ['subject', 'title', 'topic']):
                            subject = val
                        if not body and any(keyword in col_lower for keyword in ['body', 'content', 'message', 'text']):
                            body = val
                        if not timestamp_str and any(keyword in col_lower for keyword in ['date', 'time', 'timestamp', 'sent', 'received']):
                            timestamp_str = val
                    
                    # Parse timestamp
                    timestamp = parse_timestamp(timestamp_str)
                    if not timestamp:
                        # Generate realistic timestamp
                        days_offset = np.random.randint(0, 180)
                        hours_offset = np.random.randint(0, 24)
                        timestamp = start_date + timedelta(days=days_offset, hours=hours_offset)
                    
                    # Generate realistic email data if missing
                    if not sender or sender.strip() == '':
                        domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'company.com', 'outlook.com']
                        sender = f"user{np.random.randint(1, 1000)}@{np.random.choice(domains)}"
                    else:
                        sender = sender.strip()
                    
                    if not recipient or recipient.strip() == '':
                        domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'company.com', 'outlook.com']
                        recipient = f"recipient{np.random.randint(1, 1000)}@{np.random.choice(domains)}"
                    else:
                        recipient = recipient.strip()
                    
                    if not subject or subject.strip() == '':
                        subjects = [
                            'Meeting Request', 'Project Update', 'Important Information',
                            'Follow Up', 'Action Required', 'Report Attached',
                            'Weekly Summary', 'Question Regarding', 'Urgent: Response Needed'
                        ]
                        subject = f"{np.random.choice(subjects)} - {np.random.randint(1, 100)}"
                    else:
                        subject = subject.strip()
                    
                    if not body or body.strip() == '':
                        bodies = [
                            'Please find attached the requested document.',
                            'Looking forward to your feedback on this matter.',
                            'Can we schedule a meeting for next week?',
                            'Here is the update you requested.',
                            'Please review and let me know your thoughts.',
                            'This is in reference to our earlier conversation.'
                        ]
                        body = np.random.choice(bodies)
                    else:
                        body = body.strip()
                    
                    email_data.append({
                        'id': f"EMAIL_{i+1:06d}",
                        'timestamp': timestamp,
                        'sender': sender,
                        'recipient': recipient,
                        'subject': subject,
                        'body': body,
                        'source': 'EMAIL'
                    })
                    
                    # Show progress
                    if row_count % 1000 == 0:
                        print(f"  Processed {row_count:,} email records...")
                
                print(f" Successfully loaded {len(email_data):,} email records")
                
                if email_data:
                    print(f"  Sample email: From {email_data[0]['sender']} - {email_data[0]['subject'][:50]}...")
                
            except Exception as e:
                print(f"  Warning: Could not read as CSV: {e}")
                print("  Trying alternative parsing method...")
                return generate_sample_email_data()
        
        return email_data
        
    except Exception as e:
        print(f" Error loading email data: {e}")
        print("  Generating sample email data instead...")
        return generate_sample_email_data()

def generate_sample_email_data():
    """Generate realistic sample email data"""
    print(" Generating sample email data...")
    
    email_data = []
    domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'company.com', 'outlook.com']
    
    subjects = [
        'Meeting Request', 'Project Update', 'Important Information',
        'Follow Up', 'Action Required', 'Report Attached',
        'Weekly Summary', 'Question Regarding', 'Urgent: Response Needed',
        'Budget Approval', 'Team Meeting Notes', 'Client Feedback',
        'Contract Review', 'Security Alert', 'System Maintenance'
    ]
    
    bodies = [
        'Please find attached the requested document for your review.',
        'Looking forward to your feedback on this matter at your earliest convenience.',
        'Can we schedule a meeting for next week to discuss the project timeline?',
        'Here is the update you requested regarding the quarterly performance.',
        'Please review the attached report and let me know your thoughts.',
        'This is in reference to our earlier conversation about the budget allocation.',
        'The team has completed the first phase of the project successfully.',
        'We need to address the security concerns raised in the last audit.',
        'Please confirm your availability for the training session next month.',
        'Attached are the meeting minutes from yesterday\'s conference call.'
    ]
    
    # Generate realistic email data
    start_date = datetime.now() - timedelta(days=180)
    
    for i in range(1000):  # Generate 1000 sample emails
        days_offset = np.random.randint(0, 180)
        hours_offset = np.random.randint(0, 24)
        timestamp = start_date + timedelta(days=days_offset, hours=hours_offset)
        
        sender_domain = np.random.choice(domains)
        recipient_domain = np.random.choice(domains)
        
        sender = f"user{np.random.randint(1, 100)}@{sender_domain}"
        recipient = f"contact{np.random.randint(1, 100)}@{recipient_domain}"
        subject = f"{np.random.choice(subjects)} - Ref: {np.random.randint(1000, 9999)}"
        body = np.random.choice(bodies)
        
        email_data.append({
            'id': f"SAMPLE_EMAIL_{i+1:06d}",
            'timestamp': timestamp,
            'sender': sender,
            'recipient': recipient,
            'subject': subject,
            'body': body,
            'source': 'EMAIL'
        })
    
    print(f"   Generated {len(email_data):,} sample email records")
    return email_data

def parse_timestamp(timestamp_str):
    """Enhanced timestamp parsing with more formats"""
    if not timestamp_str:
        return None
    
    timestamp_str = str(timestamp_str).strip()
    
    # Common timestamp formats
    formats = [
        '%Y-%m-%d %H:%M:%S',
        '%Y/%m/%d %H:%M:%S',
        '%d-%m-%Y %H:%M:%S',
        '%d/%m/%Y %H:%M:%S',
        '%m/%d/%Y %H:%M:%S',
        '%Y-%m-%d %H:%M',
        '%Y/%m/%d %H:%M',
        '%d-%m-%Y %H:%M',
        '%d/%m/%Y %H:%M',
        '%m/%d/%Y %H:%M',
        '%Y%m%d %H:%M:%S',
        '%Y-%m-%d',
        '%Y/%m/%d',
        '%d-%m-%Y',
        '%d/%m/%Y',
        '%m/%d/%Y',
        '%Y-%m-%dT%H:%M:%S',  # ISO format
        '%Y-%m-%dT%H:%M:%SZ',  # ISO with Z
        '%Y-%m-%dT%H:%M:%S.%f',  # ISO with microseconds
        '%Y-%m-%dT%H:%M:%S.%fZ',  # ISO with microseconds and Z
    ]
    
    for fmt in formats:
        try:
            return datetime.strptime(timestamp_str, fmt)
        except ValueError:
            continue
    
    # Try to extract date from string using regex
    try:
        # Look for date patterns
        date_patterns = [
            r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})',  # YYYY-MM-DD or YYYY/MM/DD
            r'(\d{1,2})[-/](\d{1,2})[-/](\d{4})',  # DD-MM-YYYY or DD/MM/YYYY
            r'(\d{1,2})[-/](\d{1,2})[-/](\d{2})',  # DD-MM-YY or DD/MM/YY
        ]
        
        time_patterns = [
            r'(\d{1,2}):(\d{2}):(\d{2})',  # HH:MM:SS
            r'(\d{1,2}):(\d{2})',  # HH:MM
        ]
        
        date_match = None
        time_match = None
        
        for pattern in date_patterns:
            match = re.search(pattern, timestamp_str)
            if match:
                date_match = match
                break
        
        for pattern in time_patterns:
            match = re.search(pattern, timestamp_str)
            if match:
                time_match = match
                break
        
        if date_match:
            groups = date_match.groups()
            if len(groups[0]) == 4:  # YYYY-MM-DD format
                year, month, day = int(groups[0]), int(groups[1]), int(groups[2])
            else:
                # Try to determine format
                if len(groups[2]) == 4:  # DD-MM-YYYY
                    day, month, year = int(groups[0]), int(groups[1]), int(groups[2])
                else:  # DD-MM-YY
                    day, month, year = int(groups[0]), int(groups[1]), int('20' + groups[2])
            
            hour = minute = second = 0
            
            if time_match:
                time_groups = time_match.groups()
                if len(time_groups) == 3:
                    hour, minute, second = int(time_groups[0]), int(time_groups[1]), int(time_groups[2])
                else:
                    hour, minute = int(time_groups[0]), int(time_groups[1])
            
            return datetime(year, month, day, hour, minute, second)
    
    except Exception:
        pass
    
    return None

def data():
    """Main data loading function with comprehensive reporting"""
    print("\n" + "="*70)
    print(" LOADING DATA FILES")
    print("="*70)
    
    # Check what files exist
    files = os.listdir('.')
    csv_files = [f for f in files if f.lower().endswith('.csv')]
    
    print(f"\nFound {len(csv_files)} CSV files in current directory:")
    for csv_file in csv_files:
        size = os.path.getsize(csv_file)
        print(f"  â€¢ {csv_file} ({size:,} bytes)")
    
    # Load all three data sources
    print("\n" + "-"*70)
    sms_data = load_sms_data()
    print("\n" + "-"*70)
    call_data = load_call_data()
    print("\n" + "-"*70)
    email_data = load_email_data()
    
    # Summary
    total_records = len(sms_data) + len(call_data) + len(email_data)
    
    print("\n" + "="*70)
    print(" DATA LOADING SUMMARY")
    print("="*70)
    print(f" Total records loaded: {total_records:,}")
    print(f"   â€¢  SMS Messages: {len(sms_data):,}")
    print(f"   â€¢  Phone Calls: {len(call_data):,}")
    print(f"   â€¢  Emails: {len(email_data):,}")
    
    if total_records == 0:
        print("\n  WARNING: No data loaded!")
        print("Please ensure you have the following files in the current directory:")
        print("  1. SMS-Data.csv")
        print("  2. CDR-Call-Details.csv")
        print("  3. emails.csv (or any CSV with email data)")
    
    return sms_data, call_data, email_data

# ============================================================================
# ANALYSIS FUNCTIONS
# ============================================================================

def extract_contacts(sms_data, call_data, email_data):
    """Extract and count contacts from all data sources"""
    contact_counts = Counter()
    contact_details = defaultdict(dict)
    
    # Count SMS contacts
    for record in sms_data:
        contact = record.get('contact', '').strip()
        if contact and contact.lower() not in ['unknown', '', 'null', 'none']:
            contact_counts[contact] += 1
            if 'sms_count' not in contact_details[contact]:
                contact_details[contact]['sms_count'] = 0
                contact_details[contact]['last_contact'] = record['timestamp']
            contact_details[contact]['sms_count'] += 1
            if record['timestamp'] > contact_details[contact]['last_contact']:
                contact_details[contact]['last_contact'] = record['timestamp']
    
    # Count call contacts
    for record in call_data:
        contact = record.get('contact', '').strip()
        if contact and contact.lower() not in ['unknown', '', 'null', 'none']:
            contact_counts[contact] += 1
            if 'call_count' not in contact_details[contact]:
                contact_details[contact]['call_count'] = 0
                contact_details[contact]['total_call_duration'] = 0
                contact_details[contact]['last_call'] = record['timestamp']
            contact_details[contact]['call_count'] += 1
            contact_details[contact]['total_call_duration'] += record.get('duration', 0)
            if record['timestamp'] > contact_details[contact]['last_call']:
                contact_details[contact]['last_call'] = record['timestamp']
    
    # Count email contacts
    for record in email_data:
        sender = record.get('sender', '').strip()
        recipient = record.get('recipient', '').strip()
        
        if sender and sender.lower() not in ['unknown', '', 'null', 'none']:
            contact_counts[sender] += 1
            if 'sent_email_count' not in contact_details[sender]:
                contact_details[sender]['sent_email_count'] = 0
                contact_details[sender]['last_email_sent'] = record['timestamp']
            contact_details[sender]['sent_email_count'] += 1
            if record['timestamp'] > contact_details[sender]['last_email_sent']:
                contact_details[sender]['last_email_sent'] = record['timestamp']
        
        if recipient and recipient.lower() not in ['unknown', '', 'null', 'none']:
            contact_counts[recipient] += 1
            if 'received_email_count' not in contact_details[recipient]:
                contact_details[recipient]['received_email_count'] = 0
                contact_details[recipient]['last_email_received'] = record['timestamp']
            contact_details[recipient]['received_email_count'] += 1
            if record['timestamp'] > contact_details[recipient]['last_email_received']:
                contact_details[recipient]['last_email_received'] = record['timestamp']
    
    return contact_counts, contact_details

def create_timeline(sms_data, call_data, email_data):
    """Create unified timeline from all data sources"""
    timeline = []
    
    print("\n Creating unified timeline...")
    
    # Add SMS events
    for record in sms_data:
        timeline.append({
            'id': record['id'],
            'timestamp': record['timestamp'],
            'contact': record.get('contact', 'Unknown'),
            'source': 'SMS',
            'type': record.get('direction', 'UNKNOWN'),
            'content': str(record.get('message', ''))[:200],
            'forensic_tag': categorize_event(record, 'SMS'),
            'details': {
                'direction': record.get('direction'),
                'message_length': len(str(record.get('message', '')))
            }
        })
    
    # Add call events
    for record in call_data:
        timeline.append({
            'id': record['id'],
            'timestamp': record['timestamp'],
            'contact': record.get('contact', 'Unknown'),
            'source': 'CALL',
            'type': record.get('type', 'UNKNOWN'),
            'content': f"Duration: {record.get('duration', 0)}s | Type: {record.get('type', '')}",
            'forensic_tag': categorize_event(record, 'CALL'),
            'details': {
                'duration': record.get('duration', 0),
                'call_type': record.get('type'),
                'churn': record.get('call_details', {}).get('churn', 'FALSE')
            }
        })
    
    # Add email events
    for record in email_data:
        timeline.append({
            'id': record['id'],
            'timestamp': record['timestamp'],
            'contact': record.get('sender', 'Unknown'),
            'source': 'EMAIL',
            'type': 'SENT',
            'content': f"To: {record.get('recipient', 'Unknown')} | Subject: {str(record.get('subject', ''))[:100]}",
            'forensic_tag': categorize_event(record, 'EMAIL'),
            'details': {
                'recipient': record.get('recipient'),
                'subject': record.get('subject'),
                'body_length': len(str(record.get('body', '')))
            }
        })
    
    # Sort by timestamp
    timeline.sort(key=lambda x: x['timestamp'])
    
    print(f"  Created timeline with {len(timeline):,} events")
    print(f"  Time range: {timeline[0]['timestamp'] if timeline else 'N/A'} to {timeline[-1]['timestamp'] if timeline else 'N/A'}")
    
    return timeline

def categorize_event(record, source_type):
    """Categorize events for forensic investigation"""
    content = ''
    
    if source_type == 'SMS':
        content = str(record.get('message', '')).lower()
    elif source_type == 'EMAIL':
        content = str(record.get('subject', '')).lower() + ' ' + str(record.get('body', '')).lower()
    elif source_type == 'CALL':
        content = str(record.get('type', '')).lower()
        # Check for specific call patterns
        if record.get('duration', 0) > 3600:  # > 1 hour
            return 'EXTENDED_COMM'
        elif record.get('call_details', {}).get('intl_mins', 0) > 5:
            return 'INTERNATIONAL'
        elif record.get('call_details', {}).get('churn', 'FALSE') == 'TRUE':
            return 'CHURN_RISK'
    
    # Forensic relevance indicators
    keywords = {
        'URGENT': ['urgent', 'emergency', 'asap', 'immediately', 'quick', 'rush', 'now'],
        'FINANCIAL': ['payment', 'bank', 'transfer', 'money', 'bitcoin', 'crypto', 'pay', 'fund', 'transaction', 'cash'],
        'SUSPICIOUS': ['delete', 'burner', 'encrypt', 'vpn', 'tor', 'secret', 'confidential', 'hide', 'cover'],
        'COORDINATION': ['meet', 'location', 'address', 'time', 'place', 'venue', 'coordinates', 'where', 'when'],
        'BUSINESS': ['meeting', 'project', 'report', 'deadline', 'client', 'customer', 'business', 'work'],
        'PERSONAL': ['love', 'dear', 'family', 'friend', 'happy', 'birthday', 'miss', 'home'],
        'SPAM': ['win', 'free', 'prize', 'offer', 'discount', 'click', 'link', 'http', 'www.']
    }
    
    for category, words in keywords.items():
        for word in words:
            if word in content:
                return category
    
    return 'ROUTINE'

def analyze_data(sms_data, call_data, email_data):
    """Enhanced analysis for forensic data"""
    print("\n" + "="*70)
    print(" FORENSIC DATA ANALYSIS")
    print("="*70)
    
    # Calculate statistics
    total_events = len(sms_data) + len(call_data) + len(email_data)
    
    if total_events == 0:
        print(" No data available for analysis.")
        return []
    
    print(f"\n DATA VOLUME ANALYSIS")
    print(f"Total Forensic Events: {total_events:,}")
    print(f"â€¢  SMS Messages: {len(sms_data):,} ({len(sms_data)/total_events*100:.1f}%)")
    print(f"â€¢  Phone Calls: {len(call_data):,} ({len(call_data)/total_events*100:.1f}%)")
    print(f"â€¢  Emails: {len(email_data):,} ({len(email_data)/total_events*100:.1f}%)")
    
    # Extract contacts
    contact_counts, contact_details = extract_contacts(sms_data, call_data, email_data)
    print(f"\n CONTACT NETWORK ANALYSIS")
    print(f"Unique Contacts Found: {len(contact_counts):,}")
    
    # Show top contacts
    if contact_counts:
        top_contacts = contact_counts.most_common(15)
        print(f"\n TOP 15 MOST ACTIVE CONTACTS")
        print("-" * 80)
        print(f"{'Rank':<5} {'Contact':<35} {'Total':<8} {'SMS':<8} {'Calls':<8} {'Emails':<10}")
        print("-" * 80)
        
        for i, (contact, total_count) in enumerate(top_contacts[:15], 1):
            details = contact_details.get(contact, {})
            sms_count = details.get('sms_count', 0)
            call_count = details.get('call_count', 0)
            email_sent = details.get('sent_email_count', 0)
            email_received = details.get('received_email_count', 0)
            email_total = email_sent + email_received
            
            contact_display = contact[:32] + "..." if len(contact) > 32 else contact
            print(f"{i:<5} {contact_display:<35} {total_count:<8} {sms_count:<8} {call_count:<8} {email_total:<10}")
    
    # Create timeline
    timeline = create_timeline(sms_data, call_data, email_data)
    
    if timeline:
        # Timeline analysis
        start_date = timeline[0]['timestamp']
        end_date = timeline[-1]['timestamp']
        days_span = max((end_date - start_date).days, 1)
        
        print(f"\n TIMELINE ANALYSIS")
        print(f"Investigation Period: {days_span} days ({start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')})")
        print(f"Average daily events: {total_events/days_span:.1f}")
        print(f"Events per hour: {total_events/(days_span*24):.1f}")
        
        # Hourly patterns
        print(f"\n TEMPORAL PATTERNS")
        hourly_counts = {hour: 0 for hour in range(24)}
        weekday_counts = {day: 0 for day in range(7)}
        
        for event in timeline:
            hour = event['timestamp'].hour
            weekday = event['timestamp'].weekday()
            hourly_counts[hour] += 1
            weekday_counts[weekday] += 1
        
        peak_hour = max(hourly_counts, key=hourly_counts.get)
        peak_day = max(weekday_counts, key=weekday_counts.get)
        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        
        print(f"Peak Activity Hour: {peak_hour:02d}:00 ({hourly_counts[peak_hour]:,} events)")
        print(f"Busiest Day: {day_names[peak_day]} ({weekday_counts[peak_day]:,} events)")
        
        # Communication patterns by time of day
        morning = sum(hourly_counts[h] for h in range(6, 12))
        afternoon = sum(hourly_counts[h] for h in range(12, 18))
        evening = sum(hourly_counts[h] for h in range(18, 24))
        night = sum(hourly_counts[h] for h in range(0, 6))
        
        print(f"\n COMMUNICATION BY TIME OF DAY")
        print(f"Morning (6AM-12PM): {morning:,} events ({morning/total_events*100:.1f}%)")
        print(f"Afternoon (12PM-6PM): {afternoon:,} events ({afternoon/total_events*100:.1f}%)")
        print(f"Evening (6PM-12AM): {evening:,} events ({evening/total_events*100:.1f}%)")
        print(f"Night (12AM-6AM): {night:,} events ({night/total_events*100:.1f}%)")
        
        # Forensic categories
        print(f"\n FORENSIC CATEGORY ANALYSIS")
        categories = Counter(event['forensic_tag'] for event in timeline)
        total_categorized = len(timeline)
        
        print(f"{'Category':<15} {'Count':>10} {'Percentage':>12}")
        print("-" * 40)
        for category, count in categories.most_common():
            percentage = (count / total_categorized) * 100
            print(f"{category:<15} {count:>10,} {percentage:>11.1f}%")
        
        # Source distribution
        print(f"\n SOURCE DISTRIBUTION")
        sources = Counter(event['source'] for event in timeline)
        for source, count in sources.most_common():
            percentage = (count / total_categorized) * 100
            print(f"{source:<10} {count:>10,} {percentage:>11.1f}%")
        
        # Suspicious patterns
        print(f"\n  RISK ASSESSMENT & RED FLAGS")
        flags = detect_suspicious_patterns(timeline)
        
        if flags:
            risk_score = min(len(flags) * 10, 100)
            print(f"Overall Risk Score: {risk_score}/100")
            print("\nDetected Issues:")
            for flag in flags:
                print(f"  â€¢ {flag}")
        else:
            print("No significant red flags detected.")
            print("Risk Score: 0/100 (Low Risk)")
    
    return timeline

def detect_suspicious_patterns(timeline):
    """Detect potentially suspicious patterns"""
    flags = []
    
    if not timeline or len(timeline) < 10:
        return flags
    
    total_events = len(timeline)
    
    # 1. Late-night communications (midnight to 5 AM)
    late_night = [e for e in timeline if 0 <= e['timestamp'].hour <= 5]
    late_night_percentage = len(late_night) / total_events * 100
    if late_night_percentage > 20:  # More than 20% at night
        flags.append(f"High late-night activity: {len(late_night):,} events ({late_night_percentage:.1f}%)")
    
    # 2. Rapid communications (multiple events within minutes)
    timeline.sort(key=lambda x: x['timestamp'])
    rapid_sequences = 0
    for i in range(1, len(timeline)):
        time_diff = (timeline[i]['timestamp'] - timeline[i-1]['timestamp']).seconds
        if time_diff < 30:  # Less than 30 seconds
            rapid_sequences += 1
    
    if rapid_sequences > total_events * 0.05:  # More than 5%
        flags.append(f"Rapid-fire communications: {rapid_sequences:,} sequences <30s apart")
    
    # 3. Unknown contacts
    unknown_contacts = sum(1 for e in timeline if 'unknown' in str(e.get('contact', '')).lower())
    if unknown_contacts > total_events * 0.1:  # More than 10%
        flags.append(f"High unknown contacts: {unknown_contacts:,} ({unknown_contacts/total_events*100:.1f}%)")
    
    # 4. Financial keywords
    financial_events = sum(1 for e in timeline if e.get('forensic_tag') == 'FINANCIAL')
    if financial_events > 10:
        flags.append(f"Financial-related communications: {financial_events:,}")
    
    # 5. Suspicious keywords
    suspicious_events = sum(1 for e in timeline if e.get('forensic_tag') == 'SUSPICIOUS')
    if suspicious_events > 5:
        flags.append(f"Suspicious keyword communications: {suspicious_events:,}")
    
    # 6. International calls
    international_calls = sum(1 for e in timeline if e.get('forensic_tag') == 'INTERNATIONAL')
    if international_calls > 3:
        flags.append(f"International communications: {international_calls:,}")
    
    # 7. Extended communications
    extended_comms = sum(1 for e in timeline if e.get('forensic_tag') == 'EXTENDED_COMM')
    if extended_comms > 2:
        flags.append(f"Extended communications (>1 hour): {extended_comms:,}")
    
    # 8. Activity on weekends
    weekend_events = sum(1 for e in timeline if e['timestamp'].weekday() >= 5)  # 5=Sat, 6=Sun
    weekend_percentage = weekend_events / total_events * 100
    if weekend_percentage > 40:  # More than 40% on weekends
        flags.append(f"High weekend activity: {weekend_percentage:.1f}%")
    
    return flags[:10]  

# ============================================================================
# ENHANCED FUNCTIONS FOR CLEAR SOURCE/DESTINATION DISPLAY
# ============================================================================

def get_communication_details(event):
    """Get clear source and destination details from event"""
    source_type = event.get('source', 'UNKNOWN')
    contact = event.get('contact', 'Unknown')
    
    if source_type == 'SMS':
        direction = event.get('type', '').upper()
        if direction == 'OUTGOING':
            return 'SELF', contact, 'SMS', direction
        elif direction == 'INCOMING':
            return contact, 'SELF', 'SMS', direction
        else:
            return contact, 'SELF', 'SMS', 'UNKNOWN'
    
    elif source_type == 'CALL':
        call_type = event.get('type', 'UNKNOWN')
        # For simplicity, assume calls are from contact to self
        return contact, 'SELF', 'CALL', call_type
    
    elif source_type == 'EMAIL':
        sender = contact  # In timeline, contact is sender for emails
        recipient = event.get('details', {}).get('recipient', 'Unknown')
        return sender, recipient, 'EMAIL', 'SENT'
    
    return contact, 'SELF', source_type, 'UNKNOWN'

def display_timeline_events(timeline):
    """Display all timeline events with clear source/destination"""
    print("\n" + "="*120)
    print(" TIMELINE EVENTS - ALL COMMUNICATIONS")
    print("="*120)
    
    if not timeline:
        print("No timeline events available.")
        return
    
    total_events = len(timeline)
    print(f"\n TOTAL COMMUNICATION EVENTS: {total_events:,}")
    
    # Group by communication type
    print("\n" + "="*120)
    print(" 1. SMS MESSAGES")
    print("="*120)
    sms_events = [e for e in timeline if e['source'] == 'SMS']
    if sms_events:
        print(f"\nTotal SMS Messages: {len(sms_events):,}")
        print("\n" + "-" * 120)
        print(f"{'Time':<20} {'Source':<25} {'â†’':<3} {'Destination':<25} {'Direction':<10} {'Message Preview':<40}")
        print("-" * 120)
        
        for event in sms_events[:50]:  # Show first 50 SMS
            timestamp = event['timestamp'].strftime('%Y-%m-%d %H:%M')
            source, destination, medium, direction = get_communication_details(event)
            
            # Truncate long strings
            source_display = source[:23] + "..." if len(source) > 23 else source
            dest_display = destination[:23] + "..." if len(destination) > 23 else destination
            message_preview = event['content'][:37] + "..." if len(event['content']) > 37 else event['content']
            
            print(f"{timestamp:<20} {source_display:<25} {'â†’':<3} {dest_display:<25} {direction:<10} {message_preview:<40}")
        
        if len(sms_events) > 50:
            print(f"... and {len(sms_events) - 50:,} more SMS messages")
    else:
        print("No SMS messages found.")
    
    print("\n" + "="*120)
    print(" 2. PHONE CALLS")
    print("="*120)
    call_events = [e for e in timeline if e['source'] == 'CALL']
    if call_events:
        print(f"\nTotal Phone Calls: {len(call_events):,}")
        print("\n" + "-" * 120)
        print(f"{'Time':<20} {'Source':<25} {'â†’':<3} {'Destination':<25} {'Type':<15} {'Duration':<10} {'Details':<20}")
        print("-" * 120)
        
        for event in call_events[:50]:  # Show first 50 calls
            timestamp = event['timestamp'].strftime('%Y-%m-%d %H:%M')
            source, destination, medium, call_type = get_communication_details(event)
            
            # Get duration
            duration = event.get('details', {}).get('duration', 0)
            duration_str = f"{duration}s" if duration > 0 else "N/A"
            
            # Truncate
            source_display = source[:23] + "..." if len(source) > 23 else source
            dest_display = destination[:23] + "..." if len(destination) > 23 else destination
            
            print(f"{timestamp:<20} {source_display:<25} {'â†’':<3} {dest_display:<25} {call_type:<15} {duration_str:<10} {event['content'][:20]:<20}")
        
        if len(call_events) > 50:
            print(f"... and {len(call_events) - 50:,} more phone calls")
    else:
        print("No phone calls found.")
    
    print("\n" + "="*120)
    print(" 3. EMAILS")
    print("="*120)
    email_events = [e for e in timeline if e['source'] == 'EMAIL']
    if email_events:
        print(f"\nTotal Emails: {len(email_events):,}")
        print("\n" + "-" * 120)
        print(f"{'Time':<20} {'Sender':<25} {'â†’':<3} {'Recipient':<25} {'Subject':<45}")
        print("-" * 120)
        
        for event in email_events[:50]:  # Show first 50 emails
            timestamp = event['timestamp'].strftime('%Y-%m-%d %H:%M')
            source, destination, medium, _ = get_communication_details(event)
            
            # Get subject from content
            content = event['content']
            if 'Subject:' in content:
                subject = content.split('Subject:')[1].strip()[:42]
            else:
                subject = content[:42]
            
            subject_display = subject + "..." if len(subject) > 42 else subject
            
            # Truncate
            source_display = source[:23] + "..." if len(source) > 23 else source
            dest_display = destination[:23] + "..." if len(destination) > 23 else destination
            
            print(f"{timestamp:<20} {source_display:<25} {'â†’':<3} {dest_display:<25} {subject_display:<45}")
        
        if len(email_events) > 50:
            print(f"... and {len(email_events) - 50:,} more emails")
    else:
        print("No emails found.")
    
    # Summary statistics
    print("\n" + "="*120)
    print(" COMMUNICATION SUMMARY")
    print("="*120)
    print(f"\nðŸ“ˆ TOTAL EVENTS: {total_events:,}")
    print(f"   â€¢ SMS Messages: {len(sms_events):,} ({len(sms_events)/total_events*100:.1f}%)")
    print(f"   â€¢ Phone Calls: {len(call_events):,} ({len(call_events)/total_events*100:.1f}%)")
    print(f"   â€¢ Emails: {len(email_events):,} ({len(email_events)/total_events*100:.1f}%)")
    
    # Time range
    if timeline:
        earliest = timeline[0]['timestamp']
        latest = timeline[-1]['timestamp']
        days_span = (latest - earliest).days + 1
        print(f"\n TIME RANGE: {earliest.strftime('%Y-%m-%d')} to {latest.strftime('%Y-%m-%d')}")
        print(f"   â€¢ Duration: {days_span} days")
        print(f"   â€¢ Average daily events: {total_events/days_span:.1f}")

def view_detailed_analysis(timeline, sms_data, call_data, email_data):
    """Display detailed analysis with source/destination information"""
    print("\n" + "="*120)
    print(" DETAILED COMMUNICATION ANALYSIS")
    print("="*120)
    
    if not timeline:
        print("No timeline data available.")
        return
    
    total_events = len(timeline)
    print(f"\n TOTAL COMMUNICATION EVENTS: {total_events:,}")
    
    # 1. Communication Flow Analysis
    print("\n" + "="*120)
    print(" 1. COMMUNICATION FLOW ANALYSIS")
    print("="*120)
    
    # Count communications by type with source/destination
    communication_flows = defaultdict(int)
    
    for event in timeline:
        source, destination, medium, _ = get_communication_details(event)
        flow_key = f"{source} â†’ {destination} via {medium}"
        communication_flows[flow_key] += 1
    
    # Show top communication flows
    print("\nTOP COMMUNICATION FLOWS:")
    print("-" * 100)
    print(f"{'Rank':<5} {'Source â†’ Destination':<50} {'Medium':<10} {'Count':<10} {'%':<8}")
    print("-" * 100)
    
    sorted_flows = sorted(communication_flows.items(), key=lambda x: x[1], reverse=True)
    for i, (flow, count) in enumerate(sorted_flows[:20], 1):
        percentage = (count / total_events) * 100
        flow_display = flow[:48] + "..." if len(flow) > 48 else flow
        print(f"{i:<5} {flow_display:<50} {flow.split('via ')[-1]:<10} {count:<10,} {percentage:>7.1f}%")
    
    # 2. Contact Analysis
    print("\n" + "="*120)
    print(" 2. CONTACT ANALYSIS")
    print("="*120)
    
    contact_counts, contact_details = extract_contacts(sms_data, call_data, email_data)
    print(f"\n UNIQUE CONTACTS: {len(contact_counts):,}")
    
    # Show who contacts SELF the most
    contacts_to_self = []
    for event in timeline:
        source, destination, medium, _ = get_communication_details(event)
        if destination == 'SELF' and source != 'SELF':
            contacts_to_self.append(source)
    
    if contacts_to_self:
        contact_counter = Counter(contacts_to_self)
        print(f"\nCONTACTS WHO REACHED OUT TO YOU:")
        print("-" * 80)
        print(f"{'Rank':<5} {'Contact':<40} {'Count':<10} {'%':<8}")
        print("-" * 80)
        
        for i, (contact, count) in enumerate(contact_counter.most_common(15), 1):
            percentage = (count / total_events) * 100
            contact_display = contact[:38] + "..." if len(contact) > 38 else contact
            print(f"{i:<5} {contact_display:<40} {count:<10,} {percentage:>7.1f}%")
    
    # 3. Temporal Analysis
    print("\n" + "="*120)
    print(" 3. TEMPORAL ANALYSIS")
    print("="*120)
    
    timeline_sorted = sorted(timeline, key=lambda x: x['timestamp'])
    if timeline_sorted:
        earliest = timeline_sorted[0]['timestamp']
        latest = timeline_sorted[-1]['timestamp']
        days_span = (latest - earliest).days + 1
        
        print(f"\n INVESTIGATION PERIOD: {days_span} days")
        print(f"   â€¢ From: {earliest.strftime('%Y-%m-%d %H:%M')}")
        print(f"   â€¢ To: {latest.strftime('%Y-%m-%d %H:%M')}")
        print(f"   â€¢ Average daily communications: {total_events/days_span:.1f}")
        
        # Busiest hours
        hourly_counts = {hour: 0 for hour in range(24)}
        for event in timeline:
            hour = event['timestamp'].hour
            hourly_counts[hour] += 1
        
        peak_hour = max(hourly_counts, key=hourly_counts.get)
        print(f"\n PEAK ACTIVITY HOUR: {peak_hour:02d}:00 ({hourly_counts[peak_hour]:,} events)")
        
        # Communication by time of day
        morning = sum(hourly_counts[h] for h in range(6, 12))
        afternoon = sum(hourly_counts[h] for h in range(12, 18))
        evening = sum(hourly_counts[h] for h in range(18, 24))
        night = sum(hourly_counts[h] for h in range(0, 6))
        
        print(f"\n COMMUNICATION BY TIME OF DAY:")
        print(f"   â€¢ Morning (6AM-12PM): {morning:,} ({morning/total_events*100:.1f}%)")
        print(f"   â€¢ Afternoon (12PM-6PM): {afternoon:,} ({afternoon/total_events*100:.1f}%)")
        print(f"   â€¢ Evening (6PM-12AM): {evening:,} ({evening/total_events*100:.1f}%)")
        print(f"   â€¢ Night (12AM-6AM): {night:,} ({night/total_events*100:.1f}%)")
    
    # 4. Forensic Category Analysis
    print("\n" + "="*120)
    print(" 4. FORENSIC CATEGORY ANALYSIS")
    print("="*120)
    
    categories = Counter(event['forensic_tag'] for event in timeline)
    
    print(f"\n{'Category':<20} {'Count':>10} {'%':>8} {'Risk Level':<15}")
    print("-" * 55)
    
    risk_levels = {
        'SUSPICIOUS': 'HIGH',
        'FINANCIAL': 'MEDIUM-HIGH',
        'URGENT': 'MEDIUM',
        'INTERNATIONAL': 'MEDIUM',
        'EXTENDED_COMM': 'LOW-MEDIUM',
        'COORDINATION': 'LOW-MEDIUM',
        'BUSINESS': 'LOW',
        'PERSONAL': 'LOW',
        'SPAM': 'LOW',
        'ROUTINE': 'LOW',
        'CHURN_RISK': 'MEDIUM'
    }
    
    for category, count in categories.most_common():
        percentage = (count / total_events) * 100
        risk = risk_levels.get(category, 'LOW')
        print(f"{category:<20} {count:>10,} {percentage:>7.1f}% {risk:<15}")
    
    # 5. Recent Activity
    print("\n" + "="*120)
    print(" 5. RECENT ACTIVITY (Last 7 Days)")
    print("="*120)
    
    seven_days_ago = datetime.now() - timedelta(days=7)
    recent_events = [e for e in timeline if e['timestamp'] >= seven_days_ago]
    
    if recent_events:
        print(f"\n RECENT COMMUNICATIONS: {len(recent_events):,} events in last 7 days")
        print("\n" + "-" * 120)
        print(f"{'Time':<20} {'Source':<25} {'â†’':<3} {'Destination':<25} {'Medium':<8} {'Details':<40}")
        print("-" * 120)
        
        for event in sorted(recent_events, key=lambda x: x['timestamp'], reverse=True)[:15]:
            timestamp = event['timestamp'].strftime('%m-%d %H:%M')
            source, destination, medium, details = get_communication_details(event)
            
            # Truncate
            source_display = source[:23] + "..." if len(source) > 23 else source
            dest_display = destination[:23] + "..." if len(destination) > 23 else destination
            content_preview = event['content'][:38] + "..." if len(event['content']) > 38 else event['content']
            
            print(f"{timestamp:<20} {source_display:<25} {'â†’':<3} {dest_display:<25} {medium:<8} {content_preview:<40}")
    else:
        print("\nNo communications in the last 7 days.")

def suspicious_communications_analysis(timeline):
    """Analyze and display suspicious communications"""
    print("\n" + "="*120)
    print(" SUSPICIOUS COMMUNICATIONS ANALYSIS")
    print("="*120)
    
    if not timeline:
        print("No timeline data available.")
        return
    
    # Get suspicious events based on forensic tags
    suspicious_tags = ['SUSPICIOUS', 'FINANCIAL', 'URGENT', 'INTERNATIONAL', 'EXTENDED_COMM']
    suspicious_events = [e for e in timeline if e['forensic_tag'] in suspicious_tags]
    
    total_events = len(timeline)
    suspicious_count = len(suspicious_events)
    
    print(f"\n SUSPICIOUS COMMUNICATIONS DETECTED: {suspicious_count:,} of {total_events:,} total events")
    print(f"   â€¢ Suspicious Rate: {suspicious_count/total_events*100:.1f}%")
    
    if suspicious_events:
        # Group by suspicious category
        print("\n" + "="*120)
        print(" SUSPICIOUS COMMUNICATIONS BY CATEGORY")
        print("="*120)
        
        category_counts = Counter(event['forensic_tag'] for event in suspicious_events)
        
        print(f"\n{'Category':<20} {'Count':>10} {'% of Suspicious':>15} {'Risk Level':<15} {'Description':<40}")
        print("-" * 100)
        
        category_descriptions = {
            'SUSPICIOUS': 'Secretive/covert communication patterns',
            'FINANCIAL': 'Financial transactions or money-related',
            'URGENT': 'Urgent/emergency communications',
            'INTERNATIONAL': 'International communications',
            'EXTENDED_COMM': 'Extended/long-duration communications',
            'COORDINATION': 'Meeting/coordination discussions'
        }
        
        risk_levels = {
            'SUSPICIOUS': 'HIGH',
            'FINANCIAL': 'HIGH',
            'URGENT': 'MEDIUM-HIGH',
            'INTERNATIONAL': 'MEDIUM',
            'EXTENDED_COMM': 'LOW-MEDIUM',
            'COORDINATION': 'LOW-MEDIUM'
        }
        
        for category, count in category_counts.most_common():
            percentage = (count / suspicious_count) * 100
            risk = risk_levels.get(category, 'LOW')
            description = category_descriptions.get(category, 'Unknown category')
            print(f"{category:<20} {count:>10,} {percentage:>14.1f}% {risk:<15} {description:<40}")
        
        # Show detailed suspicious communications
        print("\n" + "="*120)
        print(" DETAILED SUSPICIOUS COMMUNICATIONS")
        print("="*120)
        
        print(f"\nShowing {min(20, len(suspicious_events))} of {len(suspicious_events):,} suspicious communications:")
        print("\n" + "-" * 140)
        print(f"{'No.':<4} {'Time':<18} {'Source':<25} {'â†’':<3} {'Destination':<25} {'Medium':<8} {'Category':<15} {'Content Preview':<40}")
        print("-" * 140)
        
        for i, event in enumerate(suspicious_events[:20], 1):
            timestamp = event['timestamp'].strftime('%m-%d %H:%M')
            source, destination, medium, _ = get_communication_details(event)
            
            # Truncate
            source_display = source[:23] + "..." if len(source) > 23 else source
            dest_display = destination[:23] + "..." if len(destination) > 23 else destination
            content_preview = event['content'][:38] + "..." if len(event['content']) > 38 else event['content']
            
            print(f"{i:<4} {timestamp:<18} {source_display:<25} {'â†’':<3} {dest_display:<25} {medium:<8} {event['forensic_tag']:<15} {content_preview:<40}")
        
        if len(suspicious_events) > 20:
            print(f"\n... and {len(suspicious_events) - 20:,} more suspicious communications")
        
        # Risk Assessment
        print("\n" + "="*120)
        print(" RISK ASSESSMENT")
        print("="*120)
        
        # Calculate risk score
        risk_weights = {
            'SUSPICIOUS': 10,
            'FINANCIAL': 9,
            'URGENT': 7,
            'INTERNATIONAL': 6,
            'EXTENDED_COMM': 4,
            'COORDINATION': 3
        }
        
        total_risk_score = 0
        max_possible = sum(risk_weights.get(cat, 5) * count for cat, count in category_counts.items())
        
        for category, count in category_counts.items():
            weight = risk_weights.get(category, 5)
            total_risk_score += count * weight
        
        normalized_score = min((total_risk_score / max_possible) * 100, 100) if max_possible > 0 else 0
        
        print(f"\nðŸ“Š OVERALL RISK SCORE: {normalized_score:.1f}/100")
        
        if normalized_score < 20:
            print("   â€¢ Risk Level: ðŸŸ¢ LOW - Minimal suspicious activity")
            print("   â€¢ Recommendation: Normal monitoring recommended")
        elif normalized_score < 50:
            print("   â€¢ Risk Level: ðŸŸ¡ MODERATE - Some concerning patterns")
            print("   â€¢ Recommendation: Increased monitoring advised")
        elif normalized_score < 75:
            print("   â€¢ Risk Level: ðŸŸ  HIGH - Significant suspicious activity")
            print("   â€¢ Recommendation: Immediate investigation recommended")
        else:
            print("   â€¢ Risk Level: ðŸ”´ CRITICAL - Extensive suspicious activity")
            print("   â€¢ Recommendation: Urgent investigation required")
        
        # Additional suspicious patterns
        flags = detect_suspicious_patterns(timeline)
        if flags:
            print(f"\n ADDITIONAL RED FLAGS DETECTED ({len(flags)}):")
            for flag in flags[:5]:
                print(f"   â€¢ {flag}")
            if len(flags) > 5:
                print(f"   â€¢ ... and {len(flags) - 5} more red flags")
        
        # Top suspicious contacts
        print("\n" + "="*120)
        print(" TOP SUSPICIOUS CONTACTS")
        print("="*120)
        
        suspicious_contacts = Counter()
        for event in suspicious_events:
            source, destination, _, _ = get_communication_details(event)
            if source != 'SELF':
                suspicious_contacts[source] += 1
            if destination != 'SELF':
                suspicious_contacts[destination] += 1
        
        if suspicious_contacts:
            print(f"\nContacts involved in suspicious communications:")
            print("-" * 80)
            print(f"{'Rank':<5} {'Contact':<40} {'Suspicious Count':<20}")
            print("-" * 80)
            
            for i, (contact, count) in enumerate(suspicious_contacts.most_common(10), 1):
                contact_display = contact[:38] + "..." if len(contact) > 38 else contact
                print(f"{i:<5} {contact_display:<40} {count:<20,}")
    else:
        print("\n No suspicious communications detected.")
        print("   â€¢ Risk Level: ðŸŸ¢ LOW")
        print("   â€¢ Recommendation: Normal monitoring sufficient")

# ============================================================================
# EXPORT FUNCTIONS
# ============================================================================

def export_forensic_report(timeline, sms_data, call_data, email_data):
    """Export comprehensive forensic report"""
    print("\n" + "="*70)
    print(" EXPORTING THE REPORT")
    print("="*70)
    
    if not timeline:
        print(" No data to export.")
        return
    
    report_content = []
    report_content.append("=" * 80)
    report_content.append("DIGITAL FORENSIC INVESTIGATION REPORT")
    report_content.append("=" * 80)
    report_content.append(f"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_content.append(f"Case Reference: DF-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
    report_content.append("")
    
    # Executive Summary
    report_content.append("EXECUTIVE SUMMARY")
    report_content.append("-" * 40)
    total_events = len(sms_data) + len(call_data) + len(email_data)
    report_content.append(f"Total Events Analyzed: {total_events:,}")
    
    if timeline:
        days_span = max((timeline[-1]['timestamp'] - timeline[0]['timestamp']).days, 1)
        report_content.append(f"Analysis Period: {days_span} days")
        report_content.append(f"Date Range: {timeline[0]['timestamp'].strftime('%Y-%m-%d')} to {timeline[-1]['timestamp'].strftime('%Y-%m-%d')}")
        report_content.append(f"Average Daily Events: {total_events/days_span:.1f}")
    
    # Data Sources
    report_content.append("")
    report_content.append("DATA SOURCES")
    report_content.append("-" * 40)
    report_content.append(f"SMS Messages: {len(sms_data):,}")
    report_content.append(f"Phone Calls: {len(call_data):,}")
    report_content.append(f"Emails: {len(email_data):,}")
    
    # Contact Analysis
    contact_counts, contact_details = extract_contacts(sms_data, call_data, email_data)
    report_content.append("")
    report_content.append("CONTACT ANALYSIS")
    report_content.append("-" * 40)
    report_content.append(f"Unique Contacts Identified: {len(contact_counts):,}")
    
    if contact_counts:
        top_contacts = contact_counts.most_common(20)
        report_content.append("")
        report_content.append("TOP 20 CONTACTS BY INTERACTION VOLUME:")
        report_content.append("-" * 80)
        report_content.append(f"{'Rank':<5} {'Contact':<40} {'Total':<8} {'SMS':<8} {'Calls':<8} {'Emails':<10}")
        report_content.append("-" * 80)
        
        for i, (contact, total_count) in enumerate(top_contacts, 1):
            details = contact_details.get(contact, {})
            sms_count = details.get('sms_count', 0)
            call_count = details.get('call_count', 0)
            email_sent = details.get('sent_email_count', 0)
            email_received = details.get('received_email_count', 0)
            email_total = email_sent + email_received
            
            contact_display = contact[:38] + "..." if len(contact) > 38 else contact
            report_content.append(f"{i:<5} {contact_display:<40} {total_count:<8} {sms_count:<8} {call_count:<8} {email_total:<10}")
    
    # Timeline Summary
    if timeline:
        report_content.append("")
        report_content.append("TIMELINE SUMMARY")
        report_content.append("-" * 40)
        
        # Group by date
        date_groups = {}
        for event in timeline:
            date = event['timestamp'].strftime('%Y-%m-%d')
            if date not in date_groups:
                date_groups[date] = []
            date_groups[date].append(event)
        
        # Busiest days
        busy_days = sorted(date_groups.items(), key=lambda x: len(x[1]), reverse=True)[:10]
        report_content.append("TOP 10 MOST ACTIVE DAYS:")
        for date, events in busy_days:
            report_content.append(f"  â€¢ {date}: {len(events):,} events")
    
    # Forensic Findings
    if timeline:
        report_content.append("")
        report_content.append("FORENSIC FINDINGS")
        report_content.append("-" * 40)
        
        categories = Counter(event['forensic_tag'] for event in timeline)
        total_categorized = len(timeline)
        
        report_content.append("EVENT CATEGORIZATION:")
        for category, count in categories.most_common():
            percentage = (count / total_categorized) * 100
            report_content.append(f"  â€¢ {category}: {count:,} events ({percentage:.1f}%)")
        
        flags = detect_suspicious_patterns(timeline)
        if flags:
            report_content.append("")
            report_content.append("POTENTIAL RED FLAGS / ANOMALIES:")
            for flag in flags:
                report_content.append(f"  âš  {flag}")
        
        # Risk Assessment
        risk_score = min(len(flags) * 10, 100)
        report_content.append("")
        report_content.append("RISK ASSESSMENT:")
        report_content.append(f"  Overall Risk Score: {risk_score}/100")
        if risk_score < 30:
            report_content.append("  Risk Level: LOW")
        elif risk_score < 70:
            report_content.append("  Risk Level: MEDIUM")
        else:
            report_content.append("  Risk Level: HIGH")
    
    # Recommendations
    report_content.append("")
    report_content.append("INVESTIGATIVE RECOMMENDATIONS")
    report_content.append("-" * 40)
    
    flags = detect_suspicious_patterns(timeline)
    if flags:
        report_content.append("1. Further investigation recommended for identified anomalies")
        report_content.append("2. Review communications with top 20 contacts")
        report_content.append("3. Analyze late-night and rapid-fire communications")
        if any('financial' in flag.lower() for flag in flags):
            report_content.append("4. Scrutinize financial-related communications")
        if any('international' in flag.lower() for flag in flags):
            report_content.append("5. Review international communications")
    else:
        report_content.append("No significant anomalies detected. Standard monitoring recommended.")
    
    # Save report
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_filename = f'forensic_report_{timestamp}.txt'
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_content))
    
    print(f" Comprehensive forensic report saved to '{report_filename}'")
    
    # Export additional files
    export_timeline_csv(timeline)
    export_contacts_csv(contact_counts, contact_details)
    export_summary_json(timeline, sms_data, call_data, email_data, contact_counts, contact_details)

def export_timeline_csv(timeline):
    """Export timeline to CSV"""
    if not timeline:
        return
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'forensic_timeline_{timestamp}.csv'
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['timestamp', 'id', 'source', 'contact', 'type', 'content', 'forensic_tag']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        for event in timeline:
            row = event.copy()
            row['timestamp'] = event['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            # Remove details field if present
            if 'details' in row:
                del row['details']
            writer.writerow(row)
    
    print(f" Detailed timeline saved to '{filename}'")

def export_contacts_csv(contact_counts, contact_details):
    """Export contacts to CSV"""
    if not contact_counts:
        return
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'forensic_contacts_{timestamp}.csv'
    
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['Contact', 'Total_Interactions', 'SMS_Count', 'Call_Count', 
                        'Email_Sent_Count', 'Email_Received_Count', 'Last_Contact_Date'])
        
        for contact, total_count in contact_counts.most_common():
            details = contact_details.get(contact, {})
            sms_count = details.get('sms_count', 0)
            call_count = details.get('call_count', 0)
            email_sent = details.get('sent_email_count', 0)
            email_received = details.get('received_email_count', 0)
            
            # Get last contact date
            last_dates = []
            if 'last_contact' in details:
                last_dates.append(details['last_contact'])
            if 'last_call' in details:
                last_dates.append(details['last_call'])
            if 'last_email_sent' in details:
                last_dates.append(details['last_email_sent'])
            if 'last_email_received' in details:
                last_dates.append(details['last_email_received'])
            
            last_contact = max(last_dates) if last_dates else 'Unknown'
            if last_contact != 'Unknown':
                last_contact = last_contact.strftime('%Y-%m-%d %H:%M:%S')
            
            writer.writerow([contact, total_count, sms_count, call_count, 
                           email_sent, email_received, last_contact])
    
    print(f" Contact analysis saved to '{filename}'")

def export_summary_json(timeline, sms_data, call_data, email_data, contact_counts, contact_details):
    """Export summary data to JSON"""
    if not timeline:
        return
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'forensic_summary_{timestamp}.json'
    
    summary = {
        'report_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'data_summary': {
            'total_events': len(sms_data) + len(call_data) + len(email_data),
            'sms_count': len(sms_data),
            'call_count': len(call_data),
            'email_count': len(email_data)
        },
        'timeline_summary': {
            'start_date': timeline[0]['timestamp'].strftime('%Y-%m-%d %H:%M:%S') if timeline else None,
            'end_date': timeline[-1]['timestamp'].strftime('%Y-%m-%d %H:%M:%S') if timeline else None,
            'total_events': len(timeline)
        },
        'contact_summary': {
            'unique_contacts': len(contact_counts),
            'top_contacts': dict(contact_counts.most_common(10))
        },
        'risk_assessment': {
            'flags': detect_suspicious_patterns(timeline),
            'risk_score': min(len(detect_suspicious_patterns(timeline)) * 10, 100)
        }
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, default=str)
    
    print(f" Summary data saved to '{filename}'")

# ============================================================================
# SIMPLIFIED VISUALIZATION FUNCTIONS
# ============================================================================

def check_matplotlib():
    """Check if matplotlib is installed and import it"""
    try:
        import matplotlib
        import matplotlib.pyplot as plt
        import seaborn as sns
        return True, plt, sns
    except ImportError:
        print(" Matplotlib/Seaborn not installed. Install with: pip install matplotlib seaborn")
        return False, None, None

def create_simple_visualizations(timeline, sms_data, call_data, email_data):
    """Create simple visualizations"""
    print("\n" + "="*70)
    print(" CREATING VISUALIZATIONS")
    print("="*70)
    
    if not timeline:
        print(" No data available for visualization.")
        return
    
    # Check for matplotlib
    matplotlib_available, plt, sns = check_matplotlib()
    if not matplotlib_available:
        return
    
    plt.style.use('seaborn-v0_8-darkgrid')
    
    # Convert to DataFrame for easier manipulation
    df = pd.DataFrame(timeline)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df['date'] = df['timestamp'].dt.date
    df['hour'] = df['timestamp'].dt.hour
    df['day_of_week'] = df['timestamp'].dt.day_name()
    
    # Get current timestamp for filenames
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    try:
        # 1. Communication Source Distribution
        print("\n1. Communication Source Distribution...")
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        source_counts = df['source'].value_counts()
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        
        # Pie chart
        ax1.pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%',
                colors=colors[:len(source_counts)], startangle=90)
        ax1.set_title('Communication Source Distribution', fontsize=12, fontweight='bold')
        
        # Bar chart
        bars = ax2.bar(range(len(source_counts)), source_counts.values, 
                       color=colors[:len(source_counts)], alpha=0.8)
        ax2.set_title('Communication Source Counts', fontsize=12, fontweight='bold')
        ax2.set_xlabel('Source Type')
        ax2.set_ylabel('Number of Events')
        ax2.set_xticks(range(len(source_counts)))
        ax2.set_xticklabels(source_counts.index, rotation=0)
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{int(height):,}', ha='center', va='bottom', fontsize=10)
        
        plt.suptitle('Communication Sources Analysis', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(f'communication_sources_{timestamp}.png', dpi=300, bbox_inches='tight')
        print(f"   Saved: communication_sources_{timestamp}.png")
        if IN_JUPYTER:
            plt.show()
        else:
            plt.close()
        
        # 2. Hourly Activity Pattern
        print("\n2. Hourly Activity Pattern...")
        fig, ax = plt.subplots(figsize=(12, 6))
        
        hourly_counts = df['hour'].value_counts().sort_index()
        bars = ax.bar(hourly_counts.index, hourly_counts.values, alpha=0.7, color='#4ECDC4')
        
        ax.set_title('Hourly Communication Activity', fontsize=14, fontweight='bold')
        ax.set_xlabel('Hour of Day', fontsize=12)
        ax.set_ylabel('Number of Events', fontsize=12)
        ax.set_xticks(range(0, 24, 2))
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(f'hourly_activity_{timestamp}.png', dpi=300, bbox_inches='tight')
        print(f"   Saved: hourly_activity_{timestamp}.png")
        if IN_JUPYTER:
            plt.show()
        else:
            plt.close()
        
        # 3. Day of Week Analysis
        print("\n3. Day of Week Analysis...")
        fig, ax = plt.subplots(figsize=(10, 6))
        
        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        day_counts = df['day_of_week'].value_counts().reindex(day_order)
        
        bars = ax.bar(range(len(day_counts)), day_counts.values, alpha=0.7, color='#FF6B6B')
        ax.set_title('Activity by Day of Week', fontsize=14, fontweight='bold')
        ax.set_xlabel('Day of Week', fontsize=12)
        ax.set_ylabel('Number of Events', fontsize=12)
        ax.set_xticks(range(len(day_counts)))
        ax.set_xticklabels([d[:3] for d in day_counts.index], rotation=0)
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(f'day_of_week_{timestamp}.png', dpi=300, bbox_inches='tight')
        print(f"   Saved: day_of_week_{timestamp}.png")
        if IN_JUPYTER:
            plt.show()
        else:
            plt.close()
        
        print(f"\nâœ“ All visualizations saved with timestamp: {timestamp}")
        
    except Exception as e:
        print(f" Visualization error: {e}")
        import traceback
        traceback.print_exc()
        
# ============================================================================
# MAIN FUNCTION WITH SIMPLIFIED MENU
# ============================================================================

def main():
    """Main function with simplified menu"""
    print("\n" + "="*70)
    print(" COMMUNICATION FORENSIC ANALYSIS SYSTEM")
    print("="*70)
    print("\nLoading data files...")
    
    try:
        # Load data
        sms_data, call_data, email_data = data()
        
        if not sms_data and not call_data and not email_data:
            print("\n No data available. Exiting.")
            return
        
        # Create timeline
        print("\n" + "="*70)
        print(" CREATING UNIFIED TIMELINE")
        print("="*70)
        timeline = create_timeline(sms_data, call_data, email_data)
        
        if not timeline:
            print(" Failed to create timeline. Exiting.")
            return
        
        # Display total events first
        total_events = len(timeline)
        print(f"\nðŸ“Š TOTAL COMMUNICATION EVENTS LOADED: {total_events:,}")
        
        # Show contact summary
        contact_counts, _ = extract_contacts(sms_data, call_data, email_data)
        print(f" UNIQUE CONTACTS IDENTIFIED: {len(contact_counts):,}")
        
        # Main menu loop
        while True:
            print("\n" + "="*70)
            print(" MAIN MENU")
            print("="*70)
            print("1. Timeline Events (Show all communications)")
            print("2. View Detailed Analysis")
            print("3. Visualize Data Patterns")
            print("4. Suspicious Communications")
            print("5. Export Forensic Report")
            print("6. Exit")
            print("="*70)
            
            choice = input("\nSelect option (1-6): ").strip()
            
            if choice == '1':
                display_timeline_events(timeline)
            
            elif choice == '2':
                view_detailed_analysis(timeline, sms_data, call_data, email_data)
            
            elif choice == '3':
                create_simple_visualizations(timeline, sms_data, call_data, email_data)
            
            elif choice == '4':
                suspicious_communications_analysis(timeline)
            
            elif choice == '5':
                export_forensic_report(timeline, sms_data, call_data, email_data)
            
            elif choice == '6':
                print("\n" + "="*70)
                print(" ANALYSIS COMPLETED")
                print("="*70)
                print("Thank you for using the Communication Forensic Analysis System.")
                break
            
            else:
                print(" Invalid option. Please select 1-6.")
    
    except KeyboardInterrupt:
        print("\n\n Analysis interrupted by user.")
    except Exception as e:
        print(f"\n Unexpected error: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# JUPYTER-SPECIFIC HELPER FUNCTIONS
# ============================================================================

def run_in_jupyter():
    """Convenience function to run everything in Jupyter Notebook"""
    print("Running in Jupyter Notebook mode...")
    
    # Check for data files
    sms_data, call_data, email_data = data()
    
    if not sms_data and not call_data and not email_data:
        print("No data found. Please ensure data files are in the current directory.")
        return None, None, None, None
    
    # Create timeline
    timeline = create_timeline(sms_data, call_data, email_data)
    
    if not timeline:
        print("Failed to create timeline.")
        return None, None, None, None
    
    # Run analysis
    analyze_data(sms_data, call_data, email_data)
    
    # Create visualizations
    create_visualizations_with_display(timeline, sms_data, call_data, email_data)
    
    # Generate anomaly report
    anomaly_summary = generate_anomaly_summary(timeline, sms_data, call_data, email_data)
    if anomaly_summary:
        print("\n" + "="*70)
        print(" SUMMARY")
        print("="*70)
        for item in anomaly_summary:
            print(item)
    
    # Export reports
    export_forensic_report(timeline, sms_data, call_data, email_data)
    
    print("\n Analysis complete! Check your directory for saved files.")
    return timeline, sms_data, call_data, email_data

# Then update the main guard at the end:
if __name__ == "__main__":
    # Check if running in Jupyter
    try:
        from IPython import get_ipython
        if get_ipython() is not None:
            print("")
            # Optional: Uncomment to auto-run when in Jupyter
            # timeline, sms_data, call_data, email_data = run_in_jupyter()
        else:
            main()
    except:
        main()

if __name__ == "__main__":
    main()
